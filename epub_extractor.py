#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import zipfile
import xml.etree.ElementTree as ET
import os
import re
import sys
import json
import argparse
import html.parser
from pathlib import Path
from urllib.parse import unquote

class TextChunker:
    """LLM ë²ˆì—­ì— ì í•©í•œ í¬ê¸°ë¡œ í…ìŠ¤íŠ¸ë¥¼ ì§€ëŠ¥ì ìœ¼ë¡œ ë¶„í• í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, max_chunk_size=3000, min_chunk_size=1000):
        self.max_chunk_size = max_chunk_size
        self.min_chunk_size = min_chunk_size
    
    def chunk_text(self, text, chapter_name=""):
        """í…ìŠ¤íŠ¸ë¥¼ ì ì ˆí•œ í¬ê¸°ì˜ ì²­í¬ë¡œ ë¶„í• """
        chunks = []
        
        # 1ë‹¨ê³„: ë¬¸ë‹¨ë³„ë¡œ ë¶„í•  ì‹œë„
        paragraphs = self._split_paragraphs(text)
        current_chunk = ""
        chunk_number = 1
        
        for paragraph in paragraphs:
            # í˜„ì¬ ì²­í¬ì— ë¬¸ë‹¨ì„ ì¶”ê°€í•´ë„ í¬ê¸°ê°€ ì ë‹¹í•œ ê²½ìš°
            if len(current_chunk + "\n\n" + paragraph) <= self.max_chunk_size:
                if current_chunk:
                    current_chunk += "\n\n" + paragraph
                else:
                    current_chunk = paragraph
            else:
                # í˜„ì¬ ì²­í¬ ì €ì¥ (í¬ê¸°ê°€ ì¶©ë¶„í•œ ê²½ìš°)
                if len(current_chunk) >= self.min_chunk_size:
                    chunks.append({
                        'content': current_chunk.strip(),
                        'name': f"{chapter_name}_part_{chunk_number:02d}",
                        'size': len(current_chunk)
                    })
                    chunk_number += 1
                    current_chunk = paragraph
                else:
                    # í¬ê¸°ê°€ ì‘ìœ¼ë©´ í˜„ì¬ ë¬¸ë‹¨ê³¼ í•©ì³ì„œ ê³„ì†
                    current_chunk += "\n\n" + paragraph
                
                # ë¬¸ë‹¨ì´ ë„ˆë¬´ í° ê²½ìš° ë¬¸ì¥ë³„ë¡œ ë¶„í• 
                if len(current_chunk) > self.max_chunk_size:
                    sentence_chunks = self._split_by_sentences(current_chunk, chapter_name, chunk_number)
                    chunks.extend(sentence_chunks)
                    chunk_number += len(sentence_chunks)
                    current_chunk = ""
        
        # ë§ˆì§€ë§‰ ì²­í¬ ì²˜ë¦¬
        if current_chunk.strip():
            if len(current_chunk) >= self.min_chunk_size or not chunks:
                chunks.append({
                    'content': current_chunk.strip(),
                    'name': f"{chapter_name}_part_{chunk_number:02d}",
                    'size': len(current_chunk)
                })
            else:
                # ë§ˆì§€ë§‰ ì²­í¬ê°€ ë„ˆë¬´ ì‘ìœ¼ë©´ ì´ì „ ì²­í¬ì™€ ë³‘í•©
                if chunks:
                    chunks[-1]['content'] += "\n\n" + current_chunk.strip()
                    chunks[-1]['size'] = len(chunks[-1]['content'])
        
        return chunks
    
    def _split_paragraphs(self, text):
        """í…ìŠ¤íŠ¸ë¥¼ ë¬¸ë‹¨ë³„ë¡œ ë¶„í• """
        # ì—°ì†ëœ ì¤„ë°”ê¿ˆì„ ë¬¸ë‹¨ êµ¬ë¶„ìë¡œ ì‚¬ìš©
        paragraphs = re.split(r'\n\s*\n', text.strip())
        return [p.strip() for p in paragraphs if p.strip()]
    
    def _split_by_sentences(self, text, chapter_name, start_chunk_num):
        """ê¸´ ë¬¸ë‹¨ì„ ë¬¸ì¥ë³„ë¡œ ë¶„í• """
        chunks = []
        # ë¬¸ì¥ êµ¬ë¶„ì: . ! ? ë’¤ì— ê³µë°±ì´ë‚˜ ì¤„ë°”ê¿ˆ
        sentences = re.split(r'([.!?])\s+', text)
        
        current_chunk = ""
        chunk_number = start_chunk_num
        
        for i in range(0, len(sentences)-1, 2):
            sentence = sentences[i] + (sentences[i+1] if i+1 < len(sentences) else "")
            
            if len(current_chunk + " " + sentence) <= self.max_chunk_size:
                if current_chunk:
                    current_chunk += " " + sentence
                else:
                    current_chunk = sentence
            else:
                if current_chunk:
                    chunks.append({
                        'content': current_chunk.strip(),
                        'name': f"{chapter_name}_part_{chunk_number:02d}",
                        'size': len(current_chunk)
                    })
                    chunk_number += 1
                
                # ë¬¸ì¥ì´ ì—¬ì „íˆ ë„ˆë¬´ ê¸´ ê²½ìš° ë‹¨ì–´ë³„ë¡œ ë¶„í• 
                if len(sentence) > self.max_chunk_size:
                    word_chunks = self._split_by_words(sentence, chapter_name, chunk_number)
                    chunks.extend(word_chunks)
                    chunk_number += len(word_chunks)
                    current_chunk = ""
                else:
                    current_chunk = sentence
        
        # ë§ˆì§€ë§‰ ì²­í¬ ì²˜ë¦¬
        if current_chunk.strip():
            chunks.append({
                'content': current_chunk.strip(),
                'name': f"{chapter_name}_part_{chunk_number:02d}",
                'size': len(current_chunk)
            })
        
        return chunks
    
    def _split_by_words(self, text, chapter_name, start_chunk_num):
        """ê¸´ ë¬¸ì¥ì„ ë‹¨ì–´ë³„ë¡œ ë¶„í•  (ìµœí›„ ìˆ˜ë‹¨)"""
        chunks = []
        words = text.split()
        current_chunk = ""
        chunk_number = start_chunk_num
        
        for word in words:
            if len(current_chunk + " " + word) <= self.max_chunk_size:
                if current_chunk:
                    current_chunk += " " + word
                else:
                    current_chunk = word
            else:
                if current_chunk:
                    chunks.append({
                        'content': current_chunk.strip(),
                        'name': f"{chapter_name}_part_{chunk_number:02d}",
                        'size': len(current_chunk)
                    })
                    chunk_number += 1
                current_chunk = word
        
        if current_chunk.strip():
            chunks.append({
                'content': current_chunk.strip(),
                'name': f"{chapter_name}_part_{chunk_number:02d}",
                'size': len(current_chunk)
            })
        
        return chunks


class ImprovedHTMLParser(html.parser.HTMLParser):
    """ë¬¸ë‹¨ êµ¬ì¡°ë¥¼ ë³´ì¡´í•˜ëŠ” HTML íŒŒì„œ"""
    
    def __init__(self):
        super().__init__()
        self.text_content = []
        self.current_paragraph = []
        self.in_paragraph = False
        self.skip_tags = {'script', 'style', 'head', 'title'}
        self.current_tag = None
    
    def handle_starttag(self, tag, attrs):
        if tag.lower() in self.skip_tags:
            self.current_tag = tag.lower()
            return
            
        if tag.lower() in {'p', 'div', 'section', 'article', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6'}:
            if self.current_paragraph:
                self.text_content.append(' '.join(self.current_paragraph))
                self.current_paragraph = []
            self.in_paragraph = True
        elif tag.lower() == 'br':
            if self.current_paragraph:
                self.text_content.append(' '.join(self.current_paragraph))
                self.current_paragraph = []
    
    def handle_endtag(self, tag):
        if tag.lower() in self.skip_tags:
            self.current_tag = None
            return
            
        if tag.lower() in {'p', 'div', 'section', 'article', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6'}:
            if self.current_paragraph:
                self.text_content.append(' '.join(self.current_paragraph))
                self.current_paragraph = []
            self.in_paragraph = False
    
    def handle_data(self, data):
        if self.current_tag in self.skip_tags:
            return
            
        text = data.strip()
        if text:
            self.current_paragraph.append(text)
    
    def get_text(self):
        # ë‚¨ì€ í…ìŠ¤íŠ¸ ì²˜ë¦¬
        if self.current_paragraph:
            self.text_content.append(' '.join(self.current_paragraph))
        
        # ë¬¸ë‹¨ë“¤ì„ ì´ì¤‘ ì¤„ë°”ê¿ˆìœ¼ë¡œ ì—°ê²°
        result = '\n\n'.join(self.text_content)
        
        # HTML ì—”í‹°í‹° ë””ì½”ë”©
        result = result.replace('&nbsp;', ' ')
        result = result.replace('&lt;', '<')
        result = result.replace('&gt;', '>')
        result = result.replace('&amp;', '&')
        result = result.replace('&quot;', '"')
        result = result.replace('&#39;', "'")
        
        return result


class EPUBExtractor:
    def __init__(self, epub_path, max_chunk_size=3000, min_chunk_size=1000, create_chunks=True):
        self.epub_path = epub_path
        self.zip_file = None
        self.opf_path = None
        self.chapters = []
        self.metadata = {}
        self.max_chunk_size = max_chunk_size
        self.min_chunk_size = min_chunk_size
        self.create_chunks = create_chunks
        self.chunker = TextChunker(max_chunk_size, min_chunk_size) if create_chunks else None
        
    def extract(self, output_dir=None):
        """EPUB íŒŒì¼ì„ ì±•í„°ë³„ë¡œ ë¶„ë¦¬"""
        if output_dir is None:
            output_dir = Path(self.epub_path).stem
            
        try:
            # EPUB íŒŒì¼ ì—´ê¸°
            self.zip_file = zipfile.ZipFile(self.epub_path, 'r')
            
            # OPF íŒŒì¼ ê²½ë¡œ ì°¾ê¸°
            self._find_opf_path()
            
            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            self._extract_metadata()
            
            # ëª©ì°¨ ì¶”ì¶œ
            self._extract_toc()
            
            # ë¶ˆí•„ìš”í•œ ì±•í„° í•„í„°ë§
            self._filter_chapters()
            
            # ì¶œë ¥ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±
            self._create_output_structure(output_dir)
            
            # ë©”íƒ€ë°ì´í„° ì €ì¥
            self._save_metadata(output_dir)
            
            # ì±•í„°ë³„ íŒŒì¼ ìƒì„±
            self._create_chapter_files(output_dir)
            
            # ì²­í¬ íŒŒì¼ ìƒì„± (ì˜µì…˜)
            if self.create_chunks:
                self._create_chunk_files(output_dir)
            
            print(f"âœ… ì¶”ì¶œ ì™„ë£Œ: {len(self.chapters)}ê°œ ì±•í„°")
            if self.create_chunks:
                print(f"   ğŸ“ chapters/ : ì›ë³¸ ì±•í„° íŒŒì¼ë“¤")
                print(f"   ğŸ“ chunks/   : LLM ë²ˆì—­ìš© ì²­í¬ íŒŒì¼ë“¤")
            print(f"   ğŸ“„ info.json : ì±… ì •ë³´")
            
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        finally:
            if self.zip_file:
                self.zip_file.close()
    
    def _find_opf_path(self):
        """container.xmlì—ì„œ OPF íŒŒì¼ ê²½ë¡œ ì°¾ê¸°"""
        container_xml = self.zip_file.read('META-INF/container.xml')
        root = ET.fromstring(container_xml)
        
        # namespace ì²˜ë¦¬
        ns = {'container': 'urn:oasis:names:tc:opendocument:xmlns:container'}
        rootfile = root.find('.//container:rootfile', ns)
        
        if rootfile is not None:
            self.opf_path = rootfile.get('full-path')
        else:
            # fallback: ì¼ë°˜ì ì¸ ê²½ë¡œë“¤ ì‹œë„
            possible_paths = ['OEBPS/content.opf', 'content.opf', 'EPUB/content.opf']
            for path in possible_paths:
                if path in self.zip_file.namelist():
                    self.opf_path = path
                    break
    
    def _extract_metadata(self):
        """OPF íŒŒì¼ì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        if not self.opf_path:
            raise Exception("OPF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
        opf_content = self.zip_file.read(self.opf_path)
        root = ET.fromstring(opf_content)
        
        # namespace ì²˜ë¦¬
        ns = {
            'opf': 'http://www.idpf.org/2007/opf',
            'dc': 'http://purl.org/dc/elements/1.1/'
        }
        
        # ê¸°ë³¸ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        self.metadata = {
            'title': '',
            'author': '',
            'language': '',
            'publisher': '',
            'date': '',
            'description': '',
            'epub_file': os.path.basename(self.epub_path)
        }
        
        # Dublin Core ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        title_elem = root.find('.//dc:title', ns)
        if title_elem is not None:
            self.metadata['title'] = title_elem.text or ''
            
        author_elem = root.find('.//dc:creator', ns)
        if author_elem is not None:
            self.metadata['author'] = author_elem.text or ''
            
        lang_elem = root.find('.//dc:language', ns)
        if lang_elem is not None:
            self.metadata['language'] = lang_elem.text or ''
            
        publisher_elem = root.find('.//dc:publisher', ns)
        if publisher_elem is not None:
            self.metadata['publisher'] = publisher_elem.text or ''
            
        date_elem = root.find('.//dc:date', ns)
        if date_elem is not None:
            self.metadata['date'] = date_elem.text or ''
            
        desc_elem = root.find('.//dc:description', ns)
        if desc_elem is not None:
            self.metadata['description'] = desc_elem.text or ''
    
    def _extract_toc(self):
        """OPF íŒŒì¼ì—ì„œ ëª©ì°¨ ì •ë³´ ì¶”ì¶œ"""
        if not self.opf_path:
            raise Exception("OPF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
        opf_content = self.zip_file.read(self.opf_path)
        root = ET.fromstring(opf_content)
        
        # namespace ì²˜ë¦¬
        ns = {'opf': 'http://www.idpf.org/2007/opf'}
        
        # manifestì—ì„œ íŒŒì¼ ì •ë³´ ìˆ˜ì§‘
        manifest_items = {}
        for item in root.findall('.//opf:item', ns):
            item_id = item.get('id')
            href = item.get('href')
            media_type = item.get('media-type', '')
            if href and item_id:
                manifest_items[item_id] = {
                    'href': href,
                    'media_type': media_type
                }
        
        # spineì—ì„œ ì½ê¸° ìˆœì„œ í™•ì¸
        spine_items = []
        for itemref in root.findall('.//opf:itemref', ns):
            idref = itemref.get('idref')
            if idref in manifest_items:
                spine_items.append(manifest_items[idref]['href'])
        
        # ì±•í„° ì •ë³´ ìƒì„±
        base_dir = os.path.dirname(self.opf_path)
        for i, href in enumerate(spine_items, 1):
            file_path = os.path.join(base_dir, href) if base_dir else href
            
            # íŒŒì¼ëª…ì—ì„œ ì±•í„°ëª… ì¶”ì¶œ
            chapter_name = self._extract_chapter_name(file_path, i)
            
            self.chapters.append({
                'name': chapter_name,
                'file_path': file_path,
                'order': i,
                'original_filename': os.path.basename(file_path)
            })
    
    def _filter_chapters(self):
        """ë¶ˆí•„ìš”í•œ ì±•í„°ë“¤ í•„í„°ë§ (titlepage, cover ë“±)"""
        skip_patterns = [
            r'title.*page',
            r'cover',
            r'copyright',
            r'toc',
            r'table.*of.*contents',
            r'front.*matter',
            r'dedication',
            r'epigraph'
        ]
        
        filtered_chapters = []
        for chapter in self.chapters:
            filename = chapter['original_filename'].lower()
            chapter_name = chapter['name'].lower()
            
            # íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ìŠ¤í‚µí•  ì±•í„° í™•ì¸
            should_skip = False
            for pattern in skip_patterns:
                if re.search(pattern, filename) or re.search(pattern, chapter_name):
                    should_skip = True
                    print(f"â­ï¸  ê±´ë„ˆëœ€: {chapter['name']} (ë¶ˆí•„ìš”í•œ ë‚´ìš©)")
                    break
            
            if not should_skip:
                filtered_chapters.append(chapter)
        
        self.chapters = filtered_chapters
        
        # ìˆœì„œ ì¬ì •ë ¬
        for i, chapter in enumerate(self.chapters, 1):
            chapter['order'] = i
    
    def _extract_chapter_name(self, file_path, order):
        """íŒŒì¼ ê²½ë¡œì—ì„œ ì±•í„°ëª… ì¶”ì¶œ"""
        filename = os.path.basename(file_path)
        name = os.path.splitext(filename)[0]
        
        # ì¼ë°˜ì ì¸ íŒ¨í„´ë“¤ ì •ë¦¬
        name = re.sub(r'^(chapter|ch|part|section)[\s_-]*', '', name, flags=re.IGNORECASE)
        name = re.sub(r'^\d+[\s_-]*', '', name)
        
        if not name or name.isdigit():
            name = f"Chapter_{order:03d}"
        
        # íŒŒì¼ëª…ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ë¬¸ì ì œê±°
        name = re.sub(r'[<>:"/\\|?*]', '_', name)
        name = name.strip('_')
        
        return name
    
    def _create_output_structure(self, output_dir):
        """ì¶œë ¥ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±"""
        os.makedirs(output_dir, exist_ok=True)
        os.makedirs(os.path.join(output_dir, 'chapters'), exist_ok=True)
        
        if self.create_chunks:
            os.makedirs(os.path.join(output_dir, 'chunks'), exist_ok=True)
    
    def _save_metadata(self, output_dir):
        """ë©”íƒ€ë°ì´í„°ë¥¼ info.json íŒŒì¼ë¡œ ì €ì¥"""
        info_data = {
            'book_info': self.metadata,
            'extraction_info': {
                'total_chapters': len(self.chapters),
                'chunking_enabled': self.create_chunks,
                'max_chunk_size': self.max_chunk_size if self.create_chunks else None,
                'min_chunk_size': self.min_chunk_size if self.create_chunks else None
            },
            'chapters': [
                {
                    'order': ch['order'],
                    'name': ch['name'],
                    'original_filename': ch['original_filename']
                } for ch in self.chapters
            ]
        }
        
        info_path = os.path.join(output_dir, 'info.json')
        with open(info_path, 'w', encoding='utf-8') as f:
            json.dump(info_data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“„ ë©”íƒ€ë°ì´í„° ì €ì¥ë¨: info.json")
        print(f"   ğŸ“– ì œëª©: {self.metadata['title']}")
        print(f"   âœï¸  ì €ì: {self.metadata['author']}")
    
    def _create_chapter_files(self, output_dir):
        """ì±•í„°ë³„ íŒŒì¼ ìƒì„±"""
        chapters_dir = os.path.join(output_dir, 'chapters')
        
        for chapter in self.chapters:
            try:
                # EPUB ë‚´ íŒŒì¼ ì½ê¸°
                content = self.zip_file.read(chapter['file_path']).decode('utf-8')
                
                # ê°œì„ ëœ HTML íŒŒì‹±ìœ¼ë¡œ ë¬¸ë‹¨ êµ¬ì¡° ë³´ì¡´
                text_content = self._extract_text_with_structure(content)
                
                # íŒŒì¼ ì €ì¥
                output_file = os.path.join(chapters_dir, f"{chapter['name']}.txt")
                with open(output_file, 'w', encoding='utf-8') as f:
                    f.write(text_content)
                
                # ì±•í„° ê°ì²´ì— í…ìŠ¤íŠ¸ ë‚´ìš© ì €ì¥ (ì²­í‚¹ìš©)
                chapter['content'] = text_content
                
                print(f"ğŸ“„ {chapter['name']}.txt")
                
            except Exception as e:
                print(f"âš ï¸  ì±•í„° '{chapter['name']}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def _create_chunk_files(self, output_dir):
        """LLM ë²ˆì—­ìš© ì²­í¬ íŒŒì¼ë“¤ ìƒì„±"""
        chunks_dir = os.path.join(output_dir, 'chunks')
        all_chunks = []
        
        print(f"\nğŸ”„ LLM ë²ˆì—­ìš© ì²­í¬ ìƒì„± ì¤‘...")
        
        for chapter in self.chapters:
            if 'content' not in chapter:
                continue
                
            try:
                # ì±•í„°ë¥¼ ì²­í¬ë¡œ ë¶„í• 
                chunks = self.chunker.chunk_text(chapter['content'], chapter['name'])
                
                for chunk in chunks:
                    # ì²­í¬ íŒŒì¼ ì €ì¥
                    chunk_file = os.path.join(chunks_dir, f"{chunk['name']}.txt")
                    with open(chunk_file, 'w', encoding='utf-8') as f:
                        f.write(chunk['content'])
                    
                    # ì „ì²´ ì²­í¬ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
                    all_chunks.append({
                        'file': f"{chunk['name']}.txt",
                        'chapter': chapter['name'],
                        'size': chunk['size']
                    })
                
                print(f"   ğŸ“¦ {chapter['name']}: {len(chunks)}ê°œ ì²­í¬")
                
            except Exception as e:
                print(f"âš ï¸  ì±•í„° '{chapter['name']}' ì²­í‚¹ ì¤‘ ì˜¤ë¥˜: {e}")
        
        # ì²­í¬ ì¸ë±ìŠ¤ íŒŒì¼ ìƒì„±
        chunk_index = {
            'total_chunks': len(all_chunks),
            'chunk_settings': {
                'max_size': self.max_chunk_size,
                'min_size': self.min_chunk_size
            },
            'chunks': all_chunks
        }
        
        index_path = os.path.join(chunks_dir, 'chunk_index.json')
        with open(index_path, 'w', encoding='utf-8') as f:
            json.dump(chunk_index, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… ì´ {len(all_chunks)}ê°œ ì²­í¬ ìƒì„± ì™„ë£Œ")
        print(f"ğŸ“‹ ì²­í¬ ì¸ë±ìŠ¤: chunks/chunk_index.json")
    
    def _extract_text_with_structure(self, html_content):
        """ë¬¸ë‹¨ êµ¬ì¡°ë¥¼ ë³´ì¡´í•˜ë©° HTMLì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        parser = ImprovedHTMLParser()
        parser.feed(html_content)
        text = parser.get_text()
        
        # ì¶”ê°€ ì •ë¦¬
        text = re.sub(r'\n{3,}', '\n\n', text)  # ê³¼ë„í•œ ì¤„ë°”ê¿ˆ ì œê±°
        text = re.sub(r'[ \t]+', ' ', text)     # ê³¼ë„í•œ ê³µë°± ì œê±°
        
        return text.strip()


def main():
    parser = argparse.ArgumentParser(description='EPUB íŒŒì¼ì„ ì±•í„°ë³„ë¡œ ë¶„ë¦¬í•˜ê³  LLM ë²ˆì—­ìš© ì²­í¬ë¡œ ë‚˜ëˆ„ëŠ” ë„êµ¬')
    parser.add_argument('epub_file', help='ì¶”ì¶œí•  EPUB íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--max-chunk-size', type=int, default=3000, 
                       help='ìµœëŒ€ ì²­í¬ í¬ê¸° (ë¬¸ì ìˆ˜, ê¸°ë³¸ê°’: 3000)')
    parser.add_argument('--min-chunk-size', type=int, default=1000,
                       help='ìµœì†Œ ì²­í¬ í¬ê¸° (ë¬¸ì ìˆ˜, ê¸°ë³¸ê°’: 1000)')
    parser.add_argument('--no-chunks', action='store_true',
                       help='ì²­í¬ íŒŒì¼ ìƒì„±í•˜ì§€ ì•ŠìŒ (ì±•í„° íŒŒì¼ë§Œ ìƒì„±)')
    parser.add_argument('--output-dir', '-o', help='ì¶œë ¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: EPUB íŒŒì¼ëª…)')
    
    args = parser.parse_args()
    
    if not os.path.exists(args.epub_file):
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.epub_file}")
        sys.exit(1)
    
    if not args.epub_file.lower().endswith('.epub'):
        print("âŒ EPUB íŒŒì¼ì´ ì•„ë‹™ë‹ˆë‹¤.")
        sys.exit(1)
    
    # ì²­í¬ í¬ê¸° ê²€ì¦
    if args.max_chunk_size < args.min_chunk_size:
        print("âŒ ìµœëŒ€ ì²­í¬ í¬ê¸°ê°€ ìµœì†Œ ì²­í¬ í¬ê¸°ë³´ë‹¤ ì‘ìŠµë‹ˆë‹¤.")
        sys.exit(1)
    
    create_chunks = not args.no_chunks
    
    print(f"ğŸ“š EPUB ì¶”ì¶œê¸° ì‹œì‘")
    print(f"   íŒŒì¼: {args.epub_file}")
    if create_chunks:
        print(f"   ì²­í¬ í¬ê¸°: {args.min_chunk_size}-{args.max_chunk_size} ë¬¸ì")
    else:
        print(f"   ëª¨ë“œ: ì±•í„° íŒŒì¼ë§Œ ìƒì„±")
    print()
    
    extractor = EPUBExtractor(
        args.epub_file, 
        max_chunk_size=args.max_chunk_size,
        min_chunk_size=args.min_chunk_size,
        create_chunks=create_chunks
    )
    extractor.extract(args.output_dir)


if __name__ == "__main__":
    main()