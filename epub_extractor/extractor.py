#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
EPUB ì¶”ì¶œê¸° ëª¨ë“ˆ

EPUB íŒŒì¼ì„ íŒŒì‹±í•˜ì—¬ ì±•í„°ë³„ë¡œ ë¶„ë¦¬í•˜ê³  LLM ë²ˆì—­ìš© ì²­í¬ë¡œ ë‚˜ëˆ„ëŠ” í•µì‹¬ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
"""

import zipfile
import xml.etree.ElementTree as ET
import os
import json
from pathlib import Path

from .chunker import TextChunker
from .parser import extract_text_from_html
from .utils import (
    extract_chapter_name, 
    should_skip_chapter, 
    get_common_opf_paths,
    normalize_path
)


class EPUBExtractor:
    """EPUB íŒŒì¼ ì¶”ì¶œ ë° ì²˜ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, epub_path, max_chunk_size=3500, min_chunk_size=1500, create_chunks=True, extract_raw_html=False):
        """
        EPUB ì¶”ì¶œê¸° ì´ˆê¸°í™”
        
        Args:
            epub_path (str): EPUB íŒŒì¼ ê²½ë¡œ
            max_chunk_size (int): ìµœëŒ€ ì²­í¬ í¬ê¸°
            min_chunk_size (int): ìµœì†Œ ì²­í¬ í¬ê¸°
            create_chunks (bool): ì²­í¬ íŒŒì¼ ìƒì„± ì—¬ë¶€
            extract_raw_html (bool): ì›ë³¸ HTML íŒŒì¼ ì¶”ì¶œ ì—¬ë¶€
        """
        self.epub_path = epub_path
        self.zip_file = None
        self.opf_path = None
        self.chapters = []
        self.metadata = {}
        self.max_chunk_size = max_chunk_size
        self.min_chunk_size = min_chunk_size
        self.create_chunks = create_chunks
        self.extract_raw_html = extract_raw_html
        self.chunker = TextChunker(max_chunk_size, min_chunk_size) if create_chunks else None
        
    def extract(self, output_dir=None):
        """
        EPUB íŒŒì¼ì„ ì±•í„°ë³„ë¡œ ë¶„ë¦¬
        
        Args:
            output_dir (str, optional): ì¶œë ¥ ë””ë ‰í† ë¦¬. Noneì´ë©´ EPUB íŒŒì¼ëª… ì‚¬ìš©.
        """
        if output_dir is None:
            output_dir = Path(self.epub_path).stem
            
        try:
            # EPUB íŒŒì¼ ì—´ê¸°
            self.zip_file = zipfile.ZipFile(self.epub_path, 'r')
            
            # OPF íŒŒì¼ ê²½ë¡œ ì°¾ê¸°
            self._find_opf_path()
            
            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            self._extract_metadata()
            
            # ëª©ì°¨ ì¶”ì¶œ
            self._extract_toc()
            
            # ë¶ˆí•„ìš”í•œ ì±•í„° í•„í„°ë§
            self._filter_chapters()
            
            # ì¶œë ¥ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±
            self._create_output_structure(output_dir)
            
            # ë©”íƒ€ë°ì´í„° ì €ì¥
            self._save_metadata(output_dir)
            
            # ì±•í„°ë³„ íŒŒì¼ ìƒì„±
            self._create_chapter_files(output_dir)
            
            # ì²­í¬ íŒŒì¼ ìƒì„± (ì˜µì…˜)
            if self.create_chunks:
                self._create_chunk_files(output_dir)
            
            self._print_completion_summary()
            
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise
        finally:
            if self.zip_file:
                self.zip_file.close()
    
    def _find_opf_path(self):
        """container.xmlì—ì„œ OPF íŒŒì¼ ê²½ë¡œ ì°¾ê¸°"""
        try:
            container_xml = self.zip_file.read('META-INF/container.xml')
            root = ET.fromstring(container_xml)
            
            # namespace ì²˜ë¦¬
            ns = {'container': 'urn:oasis:names:tc:opendocument:xmlns:container'}
            rootfile = root.find('.//container:rootfile', ns)
            
            if rootfile is not None:
                self.opf_path = rootfile.get('full-path')
            else:
                self._find_opf_fallback()
                
        except Exception:
            self._find_opf_fallback()
    
    def _find_opf_fallback(self):
        """OPF íŒŒì¼ì„ ì¼ë°˜ì ì¸ ê²½ë¡œì—ì„œ ì°¾ê¸° (fallback)"""
        possible_paths = get_common_opf_paths()
        
        for path in possible_paths:
            if path in self.zip_file.namelist():
                self.opf_path = path
                break
        
        if not self.opf_path:
            raise Exception("OPF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    def _extract_metadata(self):
        """OPF íŒŒì¼ì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        if not self.opf_path:
            raise Exception("OPF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
        opf_content = self.zip_file.read(self.opf_path)
        root = ET.fromstring(opf_content)
        
        # namespace ì²˜ë¦¬
        ns = {
            'opf': 'http://www.idpf.org/2007/opf',
            'dc': 'http://purl.org/dc/elements/1.1/'
        }
        
        # ê¸°ë³¸ ë©”íƒ€ë°ì´í„° ì´ˆê¸°í™”
        self.metadata = {
            'title': '',
            'author': '',
            'language': '',
            'publisher': '',
            'date': '',
            'description': '',
            'epub_file': os.path.basename(self.epub_path)
        }
        
        # Dublin Core ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        metadata_mappings = [
            ('title', './/dc:title'),
            ('author', './/dc:creator'),
            ('language', './/dc:language'),
            ('publisher', './/dc:publisher'),
            ('date', './/dc:date'),
            ('description', './/dc:description')
        ]
        
        for key, xpath in metadata_mappings:
            element = root.find(xpath, ns)
            if element is not None and element.text:
                self.metadata[key] = element.text.strip()
    
    def _extract_toc(self):
        """OPF íŒŒì¼ì—ì„œ ëª©ì°¨ ì •ë³´ ì¶”ì¶œ"""
        if not self.opf_path:
            raise Exception("OPF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
        opf_content = self.zip_file.read(self.opf_path)
        root = ET.fromstring(opf_content)
        
        # namespace ì²˜ë¦¬
        ns = {'opf': 'http://www.idpf.org/2007/opf'}
        
        # manifestì—ì„œ íŒŒì¼ ì •ë³´ ìˆ˜ì§‘
        manifest_items = {}
        for item in root.findall('.//opf:item', ns):
            item_id = item.get('id')
            href = item.get('href')
            media_type = item.get('media-type', '')
            
            if href and item_id:
                manifest_items[item_id] = {
                    'href': normalize_path(href),
                    'media_type': media_type
                }
        
        # spineì—ì„œ ì½ê¸° ìˆœì„œ í™•ì¸
        spine_items = []
        for itemref in root.findall('.//opf:itemref', ns):
            idref = itemref.get('idref')
            if idref in manifest_items:
                # HTML/XHTML íŒŒì¼ë§Œ í¬í•¨
                media_type = manifest_items[idref]['media_type']
                if 'html' in media_type.lower():
                    spine_items.append(manifest_items[idref]['href'])
        
        # ì±•í„° ì •ë³´ ìƒì„±
        base_dir = os.path.dirname(self.opf_path)
        for i, href in enumerate(spine_items, 1):
            file_path = os.path.join(base_dir, href) if base_dir else href
            file_path = normalize_path(file_path)
            
            # íŒŒì¼ëª…ì—ì„œ ì±•í„°ëª… ì¶”ì¶œ
            chapter_name = extract_chapter_name(file_path, i)
            
            self.chapters.append({
                'name': chapter_name,
                'file_path': file_path,
                'order': i,
                'original_filename': os.path.basename(file_path)
            })
    
    def _filter_chapters(self):
        """ë¶ˆí•„ìš”í•œ ì±•í„°ë“¤ í•„í„°ë§ (titlepage, cover ë“±)"""
        filtered_chapters = []
        
        for chapter in self.chapters:
            filename = chapter['original_filename']
            chapter_name = chapter['name']
            
            if should_skip_chapter(filename, chapter_name):
                print(f"â­ï¸  ê±´ë„ˆëœ€: {chapter['name']} (ë¶ˆí•„ìš”í•œ ë‚´ìš©)")
            else:
                filtered_chapters.append(chapter)
        
        self.chapters = filtered_chapters
        
        # ìˆœì„œ ì¬ì •ë ¬
        for i, chapter in enumerate(self.chapters, 1):
            chapter['order'] = i
    
    def _create_output_structure(self, output_dir):
        """ì¶œë ¥ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±"""
        os.makedirs(output_dir, exist_ok=True)
        os.makedirs(os.path.join(output_dir, 'chapters'), exist_ok=True)
        
        if self.create_chunks:
            os.makedirs(os.path.join(output_dir, 'chunks'), exist_ok=True)
    
    def _save_metadata(self, output_dir):
        """ë©”íƒ€ë°ì´í„°ë¥¼ info.json íŒŒì¼ë¡œ ì €ì¥"""
        info_data = {
            'book_info': self.metadata,
            'extraction_info': {
                'total_chapters': len(self.chapters),
                'chunking_enabled': self.create_chunks,
                'max_chunk_size': self.max_chunk_size if self.create_chunks else None,
                'min_chunk_size': self.min_chunk_size if self.create_chunks else None
            },
            'chapters': [
                {
                    'order': ch['order'],
                    'name': ch['name'],
                    'original_filename': ch['original_filename']
                } for ch in self.chapters
            ]
        }
        
        info_path = os.path.join(output_dir, 'info.json')
        with open(info_path, 'w', encoding='utf-8') as f:
            json.dump(info_data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“„ ë©”íƒ€ë°ì´í„° ì €ì¥ë¨: info.json")
        if self.metadata['title']:
            print(f"   ğŸ“– ì œëª©: {self.metadata['title']}")
        if self.metadata['author']:
            print(f"   âœï¸  ì €ì: {self.metadata['author']}")
    
    def _create_chapter_files(self, output_dir):
        """ì±•í„°ë³„ íŒŒì¼ ìƒì„±"""
        chapters_dir = os.path.join(output_dir, 'chapters')
        
        for chapter in self.chapters:
            try:
                # EPUB ë‚´ íŒŒì¼ ì½ê¸°
                content = self.zip_file.read(chapter['file_path']).decode('utf-8', errors='ignore')
                
                if self.extract_raw_html:
                    # ì›ë³¸ HTML ì €ì¥
                    output_file = os.path.join(chapters_dir, f"{chapter['name']}.html")
                    with open(output_file, 'w', encoding='utf-8') as f:
                        f.write(content)
                    print(f"ğŸ“„ {chapter['name']}.html")
                else:
                    # ê°œì„ ëœ HTML íŒŒì‹±ìœ¼ë¡œ ë¬¸ë‹¨ êµ¬ì¡° ë³´ì¡´
                    text_content = extract_text_from_html(content)
                    
                    # íŒŒì¼ ì €ì¥
                    output_file = os.path.join(chapters_dir, f"{chapter['name']}.txt")
                    with open(output_file, 'w', encoding='utf-8') as f:
                        f.write(text_content)
                    
                    # ì±•í„° ê°ì²´ì— í…ìŠ¤íŠ¸ ë‚´ìš© ì €ì¥ (ì²­í‚¹ìš©)
                    chapter['content'] = text_content
                    
                    print(f"ğŸ“„ {chapter['name']}.txt")
                
            except Exception as e:
                print(f"âš ï¸  ì±•í„° '{chapter['name']}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def _create_chunk_files(self, output_dir):
        """LLM ë²ˆì—­ìš© ì²­í¬ íŒŒì¼ë“¤ ìƒì„±"""
        chunks_dir = os.path.join(output_dir, 'chunks')
        all_chunks = []
        
        print(f"\nğŸ”„ LLM ë²ˆì—­ìš© ì²­í¬ ìƒì„± ì¤‘...")
        
        for chapter in self.chapters:
            if 'content' not in chapter:
                continue
                
            try:
                # ì±•í„°ë¥¼ ì²­í¬ë¡œ ë¶„í• 
                chunks = self.chunker.chunk_text(chapter['content'], chapter['name'])
                
                for chunk in chunks:
                    # ì²­í¬ íŒŒì¼ ì €ì¥
                    chunk_file = os.path.join(chunks_dir, f"{chunk['name']}.txt")
                    with open(chunk_file, 'w', encoding='utf-8') as f:
                        f.write(chunk['content'])
                    
                    # ì „ì²´ ì²­í¬ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
                    all_chunks.append({
                        'file': f"{chunk['name']}.txt",
                        'chapter': chapter['name'],
                        'size': chunk['size']
                    })
                
                print(f"   ğŸ“¦ {chapter['name']}: {len(chunks)}ê°œ ì²­í¬")
                
            except Exception as e:
                print(f"âš ï¸  ì±•í„° '{chapter['name']}' ì²­í‚¹ ì¤‘ ì˜¤ë¥˜: {e}")
        
        # ì²­í¬ ì¸ë±ìŠ¤ íŒŒì¼ ìƒì„±
        self._create_chunk_index(chunks_dir, all_chunks)
        
        print(f"\nâœ… ì´ {len(all_chunks)}ê°œ ì²­í¬ ìƒì„± ì™„ë£Œ")
        print(f"ğŸ“‹ ì²­í¬ ì¸ë±ìŠ¤: chunks/chunk_index.json")
    
    def _create_chunk_index(self, chunks_dir, all_chunks):
        """ì²­í¬ ì¸ë±ìŠ¤ íŒŒì¼ ìƒì„±"""
        chunk_index = {
            'total_chunks': len(all_chunks),
            'chunk_settings': {
                'max_size': self.max_chunk_size,
                'min_size': self.min_chunk_size
            },
            'statistics': {
                'avg_chunk_size': sum(chunk['size'] for chunk in all_chunks) / len(all_chunks) if all_chunks else 0,
                'total_characters': sum(chunk['size'] for chunk in all_chunks)
            },
            'chunks': all_chunks
        }
        
        index_path = os.path.join(chunks_dir, 'chunk_index.json')
        with open(index_path, 'w', encoding='utf-8') as f:
            json.dump(chunk_index, f, ensure_ascii=False, indent=2)
    
    def _print_completion_summary(self):
        """ì¶”ì¶œ ì™„ë£Œ ìš”ì•½ ì¶œë ¥"""
        print(f"\nâœ… ì¶”ì¶œ ì™„ë£Œ: {len(self.chapters)}ê°œ ì±•í„°")
        
        if self.create_chunks:
            print(f"   ğŸ“ chapters/ : ì›ë³¸ ì±•í„° íŒŒì¼ë“¤")
            print(f"   ğŸ“ chunks/   : LLM ë²ˆì—­ìš© ì²­í¬ íŒŒì¼ë“¤")
        else:
            print(f"   ğŸ“ chapters/ : ì±•í„° íŒŒì¼ë“¤")
            
        print(f"   ğŸ“„ info.json : ì±… ì •ë³´")
    
    def get_chapter_count(self):
        """ì¶”ì¶œëœ ì±•í„° ìˆ˜ ë°˜í™˜"""
        return len(self.chapters)
    
    def get_metadata(self):
        """ë©”íƒ€ë°ì´í„° ë°˜í™˜"""
        return self.metadata.copy()