#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import ollama
import json
import time
import os
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
import hashlib
from functools import lru_cache

from .prompts import get_translation_prompt

class OllamaTranslator:
    """Ollama Python ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•œ ì˜ì–´â†’í•œêµ­ì–´ ë²ˆì—­ê¸°"""
    
    def __init__(self, 
                 model_name="qwen2.5:14b",
                 temperature=0.1,
                 max_retries=3,
                 genre=None,  # Noneì´ë©´ ìë™ ê°ì§€
                 max_workers=4,
                 batch_size=5,
                 enable_cache=True,
                 num_gpu_layers=None):
        """
        Args:
            model_name: ì‚¬ìš©í•  Ollama ëª¨ë¸ëª…
            temperature: ë²ˆì—­ ì¼ê´€ì„±ì„ ìœ„í•œ ë‚®ì€ ì˜¨ë„ê°’ (0.0-2.0)
            max_retries: ë²ˆì—­ ì‹¤íŒ¨ì‹œ ì¬ì‹œë„ íšŸìˆ˜
            genre: ì†Œì„¤ ì¥ë¥´ (fantasy, sci-fi, romance, mystery, general)
            max_workers: ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜ (ê¸°ë³¸ê°’: 4)
            batch_size: ë°°ì¹˜ ì²˜ë¦¬ í¬ê¸° (ê¸°ë³¸ê°’: 5)
            enable_cache: ìºì‹± í™œì„±í™” ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
            num_gpu_layers: GPUì— ë¡œë“œí•  ë ˆì´ì–´ ìˆ˜ (Noneì´ë©´ ìë™)
        """
        self.model_name = model_name
        self.temperature = temperature
        self.max_retries = max_retries
        self.genre = genre
        self.max_workers = max_workers
        self.batch_size = batch_size
        self.enable_cache = enable_cache
        self.num_gpu_layers = num_gpu_layers
        
        # Ollama í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.client = ollama.Client()
        
        # ì¥ë¥´ ì„¤ì • (ìë™ ê°ì§€ ì§€ì›)
        self.genre = genre if genre is not None else "fantasy"  # ê¸°ë³¸ê°’
        self.auto_detect_genre = genre is None  # ìë™ ê°ì§€ ì—¬ë¶€
        
        # ì¥ë¥´ë³„ ë²ˆì—­ í”„ë¡¬í”„íŠ¸ ì„¤ì •
        self.translation_prompt = get_translation_prompt(self.genre)
        
        # ìºì‹œ ì´ˆê¸°í™”
        if enable_cache:
            self.cache_lock = threading.Lock()
            self.translation_cache = {}
        
        # í†µê³„ ì¶”ì 
        self.stats_lock = threading.Lock()
        self.stats = {
            "cache_hits": 0,
            "cache_misses": 0,
            "total_translations": 0
        }
    
    def check_ollama_available(self) -> bool:
        """Ollama ì„œë¹„ìŠ¤ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
        try:
            # ê°„ë‹¨í•œ API í˜¸ì¶œë¡œ Ollama ì„œë¹„ìŠ¤ í™•ì¸
            self.client.list()
            return True
        except Exception:
            return False
    
    def detect_genre_from_text(self, text_sample: str) -> str:
        """í…ìŠ¤íŠ¸ ìƒ˜í”Œì„ ë¶„ì„í•˜ì—¬ ìë™ìœ¼ë¡œ ì¥ë¥´ë¥¼ ê°ì§€í•©ë‹ˆë‹¤"""
        try:
            # ì¥ë¥´ ê°ì§€ìš© í”„ë¡¬í”„íŠ¸
            genre_prompt = f"""ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ ì†Œì„¤ì˜ ì¥ë¥´ë¥¼ íŒë‹¨í•´ì£¼ì„¸ìš”.
ê°€ëŠ¥í•œ ì¥ë¥´: fantasy, sci-fi, romance, mystery, horror, general

í…ìŠ¤íŠ¸:
{text_sample[:1000]}

ìœ„ í…ìŠ¤íŠ¸ì˜ ì¥ë¥´ë¥¼ í•˜ë‚˜ë§Œ ì„ íƒí•˜ì—¬ ë‹µí•´ì£¼ì„¸ìš” (ë‹¨ì–´ë§Œ): """

            response = self.client.chat(
                model=self.model_name,
                messages=[{"role": "user", "content": genre_prompt}],
                options={
                    "temperature": 0.3,
                    "top_p": 0.8,
                    "num_gpu_layers": self.num_gpu_layers
                }
            )
            
            detected_genre = response['message']['content'].strip().lower()
            
            # ìœ íš¨í•œ ì¥ë¥´ì¸ì§€ í™•ì¸
            valid_genres = ["fantasy", "sci-fi", "romance", "mystery", "horror", "general"]
            if detected_genre in valid_genres:
                return detected_genre
            else:
                # ë¶€ë¶„ ë§¤ì¹­ ì‹œë„
                for genre in valid_genres:
                    if genre in detected_genre:
                        return genre
                return "general"  # ê¸°ë³¸ê°’
                
        except Exception as e:
            print(f"âš ï¸  ì¥ë¥´ ìë™ ê°ì§€ ì‹¤íŒ¨: {e}")
            return "general"  # ì‹¤íŒ¨ì‹œ ê¸°ë³¸ê°’
    
    def check_model_available(self) -> bool:
        """ì§€ì •ëœ ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
        try:
            models = self.client.list()
            model_names = []
            for model in models.get('models', []):
                # API ì‘ë‹µ êµ¬ì¡°ì— ë”°ë¼ ë‹¤ë¥¸ í•„ë“œëª… ì‹œë„
                if hasattr(model, 'model'):
                    model_names.append(model.model)
                elif 'model' in model:
                    model_names.append(model['model'])
                elif 'name' in model:
                    model_names.append(model['name'])
            return self.model_name in model_names
        except Exception as e:
            print(f"ëª¨ë¸ í™•ì¸ ì˜¤ë¥˜: {e}")
            return False
    
    def ensure_model_loaded(self) -> bool:
        """ëª¨ë¸ì´ ë¡œë“œë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  í•„ìš”ì‹œ ë¡œë“œ"""
        try:
            # ì§§ì€ í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë¡œ ëª¨ë¸ ë¡œë“œ í™•ì¸
            test_response = self.client.generate(
                model=self.model_name,
                prompt="Hello",
                options={'num_predict': 1}  # 1í† í°ë§Œ ìƒì„±
            )
            return True
        except Exception as e:
            print(f"ëª¨ë¸ ë¡œë“œ í™•ì¸ ì‹¤íŒ¨: {e}")
            return False
    
    def _get_cache_key(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ì˜ ìºì‹œ í‚¤ ìƒì„±"""
        return hashlib.md5(text.encode('utf-8')).hexdigest()
    
    def _check_cache(self, text: str) -> Optional[str]:
        """ìºì‹œì—ì„œ ë²ˆì—­ í™•ì¸"""
        if not self.enable_cache:
            return None
        
        cache_key = self._get_cache_key(text)
        with self.cache_lock:
            if cache_key in self.translation_cache:
                with self.stats_lock:
                    self.stats["cache_hits"] += 1
                return self.translation_cache[cache_key]
        
        with self.stats_lock:
            self.stats["cache_misses"] += 1
        return None
    
    def _save_to_cache(self, text: str, translation: str):
        """ë²ˆì—­ì„ ìºì‹œì— ì €ì¥"""
        if not self.enable_cache:
            return
        
        cache_key = self._get_cache_key(text)
        with self.cache_lock:
            self.translation_cache[cache_key] = translation
    
    def translate_text(self, text: str) -> Optional[str]:
        """ë‹¨ì¼ í…ìŠ¤íŠ¸ ë¸”ë¡ ë²ˆì—­"""
        if not text.strip():
            return ""
        
        # ìºì‹œ í™•ì¸
        cached_translation = self._check_cache(text)
        if cached_translation:
            return cached_translation
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = self.translation_prompt.format(text=text.strip())
        
        for attempt in range(self.max_retries):
            try:
                # ëª¨ë¸ ì˜µì…˜ ì„¤ì •
                options = {
                    'temperature': self.temperature,
                    'top_p': 0.9,
                    'top_k': 40,
                    'repeat_penalty': 1.1
                }
                
                # GPU ë ˆì´ì–´ ì„¤ì •ì´ ìˆìœ¼ë©´ ì¶”ê°€
                if self.num_gpu_layers is not None:
                    options['num_gpu'] = self.num_gpu_layers
                
                # Ollama Python í´ë¼ì´ì–¸íŠ¸ë¡œ ë²ˆì—­ ìš”ì²­
                response = self.client.generate(
                    model=self.model_name,
                    prompt=prompt,
                    options=options
                )
                
                translation = response.get('response', '').strip()
                if translation:
                    # ìºì‹œì— ì €ì¥
                    self._save_to_cache(text, translation)
                    with self.stats_lock:
                        self.stats["total_translations"] += 1
                    return translation
                else:
                    print(f"ê²½ê³ : ë¹ˆ ë²ˆì—­ ê²°ê³¼ (ì‹œë„ {attempt + 1}/{self.max_retries})")
                    
            except ollama.ResponseError as e:
                print(f"Ollama ì‘ë‹µ ì˜¤ë¥˜: {e} (ì‹œë„ {attempt + 1}/{self.max_retries})")
                if "model" in str(e).lower() and "not found" in str(e).lower():
                    print(f"ëª¨ë¸ '{self.model_name}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    print("ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: ollama list")
                    break
            except ollama.RequestError as e:
                print(f"Ollama ìš”ì²­ ì˜¤ë¥˜: {e} (ì‹œë„ {attempt + 1}/{self.max_retries})")
                if "connection" in str(e).lower():
                    print("Ollama ì„œë¹„ìŠ¤ê°€ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                    print("ì„œë¹„ìŠ¤ ì‹œì‘: ollama serve")
                    break
            except Exception as e:
                print(f"ë²ˆì—­ ì˜¤ë¥˜: {e} (ì‹œë„ {attempt + 1}/{self.max_retries})")
            
            if attempt < self.max_retries - 1:
                time.sleep(2 ** attempt)  # ì§€ìˆ˜ ë°±ì˜¤í”„
        
        print(f"ì˜¤ë¥˜: {self.max_retries}ë²ˆ ì‹œë„ í›„ ë²ˆì—­ ì‹¤íŒ¨")
        return None
    
    def translate_batch(self, texts: List[str]) -> List[Optional[str]]:
        """ë°°ì¹˜ í…ìŠ¤íŠ¸ ë²ˆì—­"""
        results = []
        for text in texts:
            result = self.translate_text(text)
            results.append(result)
        return results
    
    def _translate_chunk_worker(self, chunk_info: Dict, chunks_dir: Path, 
                                translated_chunks_dir: Path, progress: Dict, 
                                progress_file: Path, pbar: tqdm) -> Tuple[str, bool]:
        """ë‹¨ì¼ ì²­í¬ ë²ˆì—­ ì›Œì»¤"""
        chunk_file = chunk_info["file"]
        
        # ì´ë¯¸ ì™„ë£Œëœ ì²­í¬ ê±´ë„ˆë›°ê¸°
        if chunk_file in progress.get("completed", []):
            return chunk_file, True
        
        # ì²­í¬ íŒŒì¼ ì½ê¸°
        chunk_path = chunks_dir / chunk_file
        if not chunk_path.exists():
            print(f"ê²½ê³ : ì²­í¬ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {chunk_file}")
            with self.stats_lock:
                progress.setdefault("failed", []).append(chunk_file)
            return chunk_file, False
        
        try:
            with open(chunk_path, 'r', encoding='utf-8') as f:
                original_text = f.read()
            
            # ë²ˆì—­ ìˆ˜í–‰
            translated_text = self.translate_text(original_text)
            
            if translated_text:
                # ë²ˆì—­ ê²°ê³¼ ì €ì¥
                output_file = translated_chunks_dir / f"ko_{chunk_file}"
                with open(output_file, 'w', encoding='utf-8') as f:
                    f.write(translated_text)
                
                # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
                with self.stats_lock:
                    progress.setdefault("completed", []).append(chunk_file)
                    if chunk_file in progress.get("failed", []):
                        progress["failed"].remove(chunk_file)
                    pbar.update(1)
                
                return chunk_file, True
            else:
                # ì‹¤íŒ¨ ê¸°ë¡
                with self.stats_lock:
                    progress.setdefault("failed", []).append(chunk_file)
                print(f"ì‹¤íŒ¨: {chunk_file}")
                return chunk_file, False
                
        except Exception as e:
            print(f"ì˜¤ë¥˜ ì²˜ë¦¬ ì¤‘ {chunk_file}: {e}")
            with self.stats_lock:
                progress.setdefault("failed", []).append(chunk_file)
            return chunk_file, False
    
    def translate_chunks(self, input_dir: str, output_dir: str, use_parallel: bool = True) -> Dict[str, any]:
        """ì²­í¬ ë””ë ‰í† ë¦¬ ì „ì²´ ë²ˆì—­ (ë³‘ë ¬ ì²˜ë¦¬ ì§€ì›)"""
        input_path = Path(input_dir)
        output_path = Path(output_dir)
        
        # ì…ë ¥ ë””ë ‰í† ë¦¬ í™•ì¸
        if not input_path.exists():
            raise FileNotFoundError(f"ì…ë ¥ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_dir}")
        
        chunks_dir = input_path / "chunks"
        if not chunks_dir.exists():
            raise FileNotFoundError(f"ì²­í¬ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {chunks_dir}")
        
        # ì²­í¬ ì¸ë±ìŠ¤ ë¡œë“œ
        chunk_index_file = chunks_dir / "chunk_index.json"
        if not chunk_index_file.exists():
            raise FileNotFoundError(f"ì²­í¬ ì¸ë±ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {chunk_index_file}")
        
        with open(chunk_index_file, 'r', encoding='utf-8') as f:
            chunk_index = json.load(f)
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        output_path.mkdir(parents=True, exist_ok=True)
        translated_chunks_dir = output_path / "translated_chunks"
        translated_chunks_dir.mkdir(exist_ok=True)
        
        # ë²ˆì—­ ì§„í–‰ ìƒí™© íŒŒì¼
        progress_file = output_path / "translation_progress.json"
        progress = self._load_progress(progress_file)
        
        # ë²ˆì—­ í†µê³„
        stats = {
            "total_chunks": len(chunk_index["chunks"]),
            "completed": len(progress.get("completed", [])),
            "failed": len(progress.get("failed", [])),
            "start_time": time.time(),
            "model_name": self.model_name,
            "parallel_mode": use_parallel,
            "max_workers": self.max_workers if use_parallel else 1
        }
        
        # ìë™ ì¥ë¥´ ê°ì§€ (í•„ìš”í•œ ê²½ìš°)
        if self.auto_detect_genre and len(chunk_index["chunks"]) > 0:
            print("ğŸ” ì¥ë¥´ ìë™ ê°ì§€ ì¤‘...")
            
            # ì²« ë²ˆì§¸ ì²­í¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¥ë¥´ ê°ì§€
            first_chunk_file = chunk_index["chunks"][0]["file"]
            first_chunk_path = chunks_dir / first_chunk_file
            
            try:
                with open(first_chunk_path, 'r', encoding='utf-8') as f:
                    sample_text = f.read()
                
                detected_genre = self.detect_genre_from_text(sample_text)
                
                if detected_genre != self.genre:
                    print(f"ğŸ“š ê°ì§€ëœ ì¥ë¥´: {detected_genre} (ê¸°ë³¸ê°’ {self.genre}ì—ì„œ ë³€ê²½)")
                    self.genre = detected_genre
                    # í”„ë¡¬í”„íŠ¸ ì—…ë°ì´íŠ¸
                    from .prompts import get_translation_prompt
                    self.translation_prompt = get_translation_prompt(self.genre)
                else:
                    print(f"ğŸ“š ì¥ë¥´: {self.genre} (ìë™ ê°ì§€ë¡œ í™•ì¸ë¨)")
            except Exception as e:
                print(f"âš ï¸  ì¥ë¥´ ê°ì§€ ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©: {self.genre}")
        else:
            print(f"ğŸ“š ì¥ë¥´: {self.genre} (ì‚¬ìš©ì ì§€ì •)")

        print(f"ğŸ“š ë²ˆì—­ ì‹œì‘: {stats['total_chunks']}ê°œ ì²­í¬")
        print(f"ğŸ¤– ëª¨ë¸: {self.model_name}")
        print(f"âš¡ ë³‘ë ¬ ì²˜ë¦¬: {'í™œì„±í™”' if use_parallel else 'ë¹„í™œì„±í™”'} (ì›Œì»¤: {stats['max_workers']})")
        print(f"âœ… ì™„ë£Œ: {stats['completed']}ê°œ")
        print(f"âŒ ì‹¤íŒ¨: {stats['failed']}ê°œ")
        if self.enable_cache:
            print(f"ğŸ’¾ ìºì‹±: í™œì„±í™”")
        print("=" * 50)
        
        # ì§„í–‰ë°” ì„¤ì • (macOS zsh + oh-my-zsh í˜¸í™˜)
        # í„°ë¯¸ë„ í™˜ê²½ ì²´í¬
        term = os.environ.get('TERM', '')
        is_dumb_terminal = term in ['dumb', ''] or os.environ.get('CI') == 'true'
        
        pbar = tqdm(
            total=len(chunk_index["chunks"]),
            desc="ë²ˆì—­ ì§„í–‰",
            initial=stats['completed'],
            ncols=80,  # í„°ë¯¸ë„ ë„ˆë¹„ ê³ ì •
            ascii=True,  # ASCII ë¬¸ì ì‚¬ìš© (ìœ ë‹ˆì½”ë“œ ë¬¸ì œ ë°©ì§€)
            bar_format='{desc}: {percentage:3.0f}%|{bar:20}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]',
            disable=is_dumb_terminal,  # dumb í„°ë¯¸ë„ì—ì„œëŠ” ë¹„í™œì„±í™”
            dynamic_ncols=False,  # ë™ì  ë„ˆë¹„ ë¹„í™œì„±í™”
            leave=True,  # ì™„ë£Œ í›„ì—ë„ ì§„í–‰ë°” ìœ ì§€
            mininterval=0.5,  # ì—…ë°ì´íŠ¸ ìµœì†Œ ê°„ê²© (0.5ì´ˆ)
            maxinterval=2.0   # ì—…ë°ì´íŠ¸ ìµœëŒ€ ê°„ê²© (2ì´ˆ)
        )
        
        if use_parallel:
            # ë³‘ë ¬ ì²˜ë¦¬
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                # ë¯¸ì™„ë£Œ ì²­í¬ë§Œ ì¶”ì¶œ
                pending_chunks = [
                    chunk for chunk in chunk_index["chunks"]
                    if chunk["file"] not in progress.get("completed", [])
                ]
                
                # ë°°ì¹˜ë³„ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
                for i in range(0, len(pending_chunks), self.batch_size):
                    batch = pending_chunks[i:i + self.batch_size]
                    
                    # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì‘ì—… ì œì¶œ
                    futures = {
                        executor.submit(
                            self._translate_chunk_worker,
                            chunk_info, chunks_dir, translated_chunks_dir,
                            progress, progress_file, pbar
                        ): chunk_info for chunk_info in batch
                    }
                    
                    # ì™„ë£Œëœ ì‘ì—… ì²˜ë¦¬
                    for future in as_completed(futures):
                        chunk_file, success = future.result()
                        if success:
                            stats["completed"] = len(progress.get("completed", []))
                        else:
                            stats["failed"] = len(progress.get("failed", []))
                        
                        # ì§„í–‰ ìƒí™© ì €ì¥ (ë°°ì¹˜ë§ˆë‹¤)
                        self._save_progress(progress_file, progress)
        else:
            # ìˆœì°¨ ì²˜ë¦¬ (ê¸°ì¡´ ë°©ì‹)
            for chunk_info in chunk_index["chunks"]:
                chunk_file, success = self._translate_chunk_worker(
                    chunk_info, chunks_dir, translated_chunks_dir,
                    progress, progress_file, pbar
                )
                if success:
                    stats["completed"] = len(progress.get("completed", []))
                else:
                    stats["failed"] = len(progress.get("failed", []))
                
                # ì§„í–‰ ìƒí™© ì €ì¥
                self._save_progress(progress_file, progress)
        
        pbar.close()
        
        # ë²ˆì—­ ì™„ë£Œ í†µê³„
        stats["end_time"] = time.time()
        stats["duration"] = stats["end_time"] - stats["start_time"]
        
        # ìºì‹œ í†µê³„ ì¶”ê°€
        if self.enable_cache:
            stats["cache_stats"] = {
                "hits": self.stats["cache_hits"],
                "misses": self.stats["cache_misses"],
                "hit_rate": (self.stats["cache_hits"] / 
                           (self.stats["cache_hits"] + self.stats["cache_misses"]) * 100
                           if (self.stats["cache_hits"] + self.stats["cache_misses"]) > 0 else 0)
            }
        
        # ë²ˆì—­ ì¸ë±ìŠ¤ ìƒì„±
        self._create_translation_index(output_path, chunk_index, stats)
        
        return stats
    
    def _load_progress(self, progress_file: Path) -> Dict:
        """ë²ˆì—­ ì§„í–‰ ìƒí™© ë¡œë“œ"""
        if progress_file.exists():
            try:
                with open(progress_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception:
                pass
        return {"completed": [], "failed": []}
    
    def _save_progress(self, progress_file: Path, progress: Dict):
        """ë²ˆì—­ ì§„í–‰ ìƒí™© ì €ì¥"""
        try:
            with open(progress_file, 'w', encoding='utf-8') as f:
                json.dump(progress, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"ì§„í–‰ ìƒí™© ì €ì¥ ì˜¤ë¥˜: {e}")
    
    def _create_translation_index(self, output_path: Path, original_index: Dict, stats: Dict):
        """ë²ˆì—­ ì¸ë±ìŠ¤ íŒŒì¼ ìƒì„±"""
        translation_index = {
            "translation_info": {
                "model": self.model_name,
                "temperature": self.temperature,
                "start_time": time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(stats["start_time"])),
                "end_time": time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(stats["end_time"])),
                "duration_minutes": round(stats["duration"] / 60, 2),
                "total_chunks": stats["total_chunks"],
                "completed_chunks": stats["completed"],
                "failed_chunks": stats["failed"]
            },
            "original_info": original_index
        }
        
        index_file = output_path / "translation_index.json"
        with open(index_file, 'w', encoding='utf-8') as f:
            json.dump(translation_index, f, ensure_ascii=False, indent=2)

def main():
    """ë²ˆì—­ê¸° í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Ollama ë²ˆì—­ê¸°")
    parser.add_argument("input_dir", help="ì…ë ¥ ë””ë ‰í† ë¦¬ (ì²­í¬ê°€ ìˆëŠ” ë””ë ‰í† ë¦¬)")
    parser.add_argument("output_dir", help="ì¶œë ¥ ë””ë ‰í† ë¦¬")
    parser.add_argument("--model", default="llama3.1:8b", help="Ollama ëª¨ë¸ëª…")
    parser.add_argument("--temperature", type=float, default=0.1, help="ë²ˆì—­ ì˜¨ë„")
    parser.add_argument("--genre", default="fantasy", choices=["fantasy", "sci-fi", "romance", "mystery", "general"],
                       help="ì†Œì„¤ ì¥ë¥´")
    parser.add_argument("--max-workers", type=int, default=4, help="ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜")
    parser.add_argument("--batch-size", type=int, default=5, help="ë°°ì¹˜ ì²˜ë¦¬ í¬ê¸°")
    parser.add_argument("--no-parallel", action="store_true", help="ë³‘ë ¬ ì²˜ë¦¬ ë¹„í™œì„±í™”")
    parser.add_argument("--no-cache", action="store_true", help="ìºì‹± ë¹„í™œì„±í™”")
    parser.add_argument("--num-gpu-layers", type=int, help="GPUì— ë¡œë“œí•  ë ˆì´ì–´ ìˆ˜")
    
    args = parser.parse_args()
    
    # ë²ˆì—­ê¸° ì´ˆê¸°í™”
    translator = OllamaTranslator(
        model_name=args.model,
        temperature=args.temperature,
        genre=args.genre,
        max_workers=args.max_workers,
        batch_size=args.batch_size,
        enable_cache=not args.no_cache,
        num_gpu_layers=args.num_gpu_layers
    )
    
    # ì—°ê²° í™•ì¸
    if not translator.check_ollama_available():
        print("âŒ Ollama ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("Ollamaê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return
    
    if not translator.check_model_available():
        print(f"âŒ ëª¨ë¸ '{args.model}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ì„ í™•ì¸í•´ì£¼ì„¸ìš”: ollama list")
        return
    
    print("âœ… Ollama ì—°ê²° í™•ì¸ ì™„ë£Œ")
    
    # ë²ˆì—­ ìˆ˜í–‰
    try:
        stats = translator.translate_chunks(
            args.input_dir, 
            args.output_dir,
            use_parallel=not args.no_parallel
        )
        
        print("\n" + "=" * 50)
        print("ğŸ“Š ë²ˆì—­ ì™„ë£Œ!")
        print(f"ì´ ì²­í¬: {stats['total_chunks']}ê°œ")
        print(f"ì™„ë£Œ: {stats['completed']}ê°œ")
        print(f"ì‹¤íŒ¨: {stats['failed']}ê°œ")
        print(f"ì†Œìš” ì‹œê°„: {stats['duration'] / 60:.1f}ë¶„")
        if "cache_stats" in stats:
            print(f"ìºì‹œ íˆíŠ¸ìœ¨: {stats['cache_stats']['hit_rate']:.1f}%")
        print(f"ë²ˆì—­ ê²°ê³¼: {args.output_dir}")
        
    except Exception as e:
        print(f"âŒ ë²ˆì—­ ì˜¤ë¥˜: {e}")

if __name__ == "__main__":
    main()