#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import ollama
import json
import time
import os
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
import hashlib
from functools import lru_cache

from .prompts import get_translation_prompt

class OllamaTranslator:
    """Ollama Python ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•œ ì˜ì–´â†’í•œêµ­ì–´ ë²ˆì—­ê¸°"""
    
    def __init__(self, 
                 model_name="llama3-ko:8b",
                 temperature=0.1,
                 max_retries=3,
                 genre=None,  # Noneì´ë©´ ìë™ ê°ì§€
                 max_workers=4,
                 batch_size=5,
                 enable_cache=True,
                 num_gpu_layers=None):
        """
        Args:
            model_name: ì‚¬ìš©í•  Ollama ëª¨ë¸ëª…
            temperature: ë²ˆì—­ ì¼ê´€ì„±ì„ ìœ„í•œ ë‚®ì€ ì˜¨ë„ê°’ (0.0-2.0)
            max_retries: ë²ˆì—­ ì‹¤íŒ¨ì‹œ ì¬ì‹œë„ íšŸìˆ˜
            genre: ì†Œì„¤ ì¥ë¥´ (fantasy, sci-fi, romance, mystery, general)
            max_workers: ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜ (ê¸°ë³¸ê°’: 4)
            batch_size: ë°°ì¹˜ ì²˜ë¦¬ í¬ê¸° (ê¸°ë³¸ê°’: 5)
            enable_cache: ìºì‹± í™œì„±í™” ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
            num_gpu_layers: GPUì— ë¡œë“œí•  ë ˆì´ì–´ ìˆ˜ (Noneì´ë©´ ìë™)
        """
        self.model_name = model_name
        self.temperature = temperature
        self.max_retries = max_retries
        self.genre = genre
        self.max_workers = max_workers
        self.batch_size = batch_size
        self.enable_cache = enable_cache
        self.num_gpu_layers = num_gpu_layers
        
        # Ollama í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.client = ollama.Client()
        
        # ì¥ë¥´ ì„¤ì • (ìë™ ê°ì§€ ì§€ì›)
        self.genre = genre if genre is not None else "fantasy"  # ê¸°ë³¸ê°’
        self.auto_detect_genre = genre is None  # ìë™ ê°ì§€ ì—¬ë¶€
        
        # ì¥ë¥´ë³„ ë²ˆì—­ í”„ë¡¬í”„íŠ¸ ì„¤ì •
        self.translation_prompt = get_translation_prompt(self.genre)
        
        # ìºì‹œ ì´ˆê¸°í™”
        if enable_cache:
            self.cache_lock = threading.Lock()
            self.translation_cache = {}
        
        # í†µê³„ ì¶”ì 
        self.stats_lock = threading.Lock()
        self.stats = {
            "cache_hits": 0,
            "cache_misses": 0,
            "total_translations": 0
        }
    
    def check_ollama_available(self) -> bool:
        """Ollama ì„œë¹„ìŠ¤ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
        try:
            # ê°„ë‹¨í•œ API í˜¸ì¶œë¡œ Ollama ì„œë¹„ìŠ¤ í™•ì¸
            self.client.list()
            return True
        except Exception:
            return False
    
    def detect_genre_from_text(self, text_sample: str) -> str:
        """í…ìŠ¤íŠ¸ ìƒ˜í”Œì„ ë¶„ì„í•˜ì—¬ ìë™ìœ¼ë¡œ ì¥ë¥´ë¥¼ ê°ì§€í•©ë‹ˆë‹¤"""
        try:
            # ì¥ë¥´ ê°ì§€ìš© í”„ë¡¬í”„íŠ¸
            genre_prompt = f"""ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ ì†Œì„¤ì˜ ì¥ë¥´ë¥¼ íŒë‹¨í•´ì£¼ì„¸ìš”.
ê°€ëŠ¥í•œ ì¥ë¥´: fantasy, sci-fi, romance, mystery, horror, general

í…ìŠ¤íŠ¸:
{text_sample[:1000]}

ìœ„ í…ìŠ¤íŠ¸ì˜ ì¥ë¥´ë¥¼ í•˜ë‚˜ë§Œ ì„ íƒí•˜ì—¬ ë‹µí•´ì£¼ì„¸ìš” (ë‹¨ì–´ë§Œ): """

            response = self.client.chat(
                model=self.model_name,
                messages=[{"role": "user", "content": genre_prompt}],
                options={
                    "temperature": 0.3,
                    "top_p": 0.8,
                    "num_gpu_layers": self.num_gpu_layers
                }
            )
            
            detected_genre = response['message']['content'].strip().lower()
            
            # ìœ íš¨í•œ ì¥ë¥´ì¸ì§€ í™•ì¸
            valid_genres = ["fantasy", "sci-fi", "romance", "mystery", "horror", "general"]
            if detected_genre in valid_genres:
                return detected_genre
            else:
                # ë¶€ë¶„ ë§¤ì¹­ ì‹œë„
                for genre in valid_genres:
                    if genre in detected_genre:
                        return genre
                return "general"  # ê¸°ë³¸ê°’
                
        except Exception as e:
            print(f"âš ï¸  ì¥ë¥´ ìë™ ê°ì§€ ì‹¤íŒ¨: {e}")
            return "general"  # ì‹¤íŒ¨ì‹œ ê¸°ë³¸ê°’
    
    def check_model_available(self) -> bool:
        """ì§€ì •ëœ ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
        try:
            models = self.client.list()
            model_names = []
            for model in models.get('models', []):
                # API ì‘ë‹µ êµ¬ì¡°ì— ë”°ë¼ ë‹¤ë¥¸ í•„ë“œëª… ì‹œë„
                if hasattr(model, 'model'):
                    model_names.append(model.model)
                elif 'model' in model:
                    model_names.append(model['model'])
                elif 'name' in model:
                    model_names.append(model['name'])
            return self.model_name in model_names
        except Exception as e:
            print(f"ëª¨ë¸ í™•ì¸ ì˜¤ë¥˜: {e}")
            return False
    
    def ensure_model_loaded(self) -> bool:
        """ëª¨ë¸ì´ ë¡œë“œë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  í•„ìš”ì‹œ ë¡œë“œ"""
        try:
            # ì§§ì€ í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë¡œ ëª¨ë¸ ë¡œë“œ í™•ì¸
            test_response = self.client.generate(
                model=self.model_name,
                prompt="Hello",
                options={'num_predict': 1}  # 1í† í°ë§Œ ìƒì„±
            )
            return True
        except Exception as e:
            print(f"ëª¨ë¸ ë¡œë“œ í™•ì¸ ì‹¤íŒ¨: {e}")
            return False
    
    def _get_cache_key(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ì˜ ìºì‹œ í‚¤ ìƒì„±"""
        return hashlib.md5(text.encode('utf-8')).hexdigest()
    
    def _check_cache(self, text: str) -> Optional[str]:
        """ìºì‹œì—ì„œ ë²ˆì—­ í™•ì¸"""
        if not self.enable_cache:
            return None
        
        cache_key = self._get_cache_key(text)
        with self.cache_lock:
            if cache_key in self.translation_cache:
                with self.stats_lock:
                    self.stats["cache_hits"] += 1
                return self.translation_cache[cache_key]
        
        with self.stats_lock:
            self.stats["cache_misses"] += 1
        return None
    
    def _save_to_cache(self, text: str, translation: str):
        """ë²ˆì—­ì„ ìºì‹œì— ì €ì¥"""
        if not self.enable_cache:
            return
        
        cache_key = self._get_cache_key(text)
        with self.cache_lock:
            self.translation_cache[cache_key] = translation
    
    def _validate_korean_only(self, text: str) -> str:
        """ë²ˆì—­ ê²°ê³¼ê°€ í•œêµ­ì–´ë§Œ í¬í•¨í•˜ëŠ”ì§€ ê²€ì¦í•˜ê³  ì •ë¦¬"""
        import re
        
        # ì¤‘êµ­ì–´ ë¬¸ì ë²”ìœ„ ê²€ì¶œ
        chinese_pattern = r'[\u4e00-\u9fff]'
        # ì¼ë³¸ì–´ ë¬¸ì ë²”ìœ„ ê²€ì¶œ (íˆë¼ê°€ë‚˜, ê°€íƒ€ì¹´ë‚˜, í•œì)
        japanese_pattern = r'[\u3040-\u309f\u30a0-\u30ff]'
        # íŠ¹ìˆ˜ ë¬¸ì íŒ¨í„´ ê²€ì¶œ (&O;, &C; ë“±)
        special_entity_pattern = r'&[A-Z]+;'
        # HTML ì—”í‹°í‹° ê²€ì¶œ
        html_entity_pattern = r'&[a-zA-Z0-9#]+;'
        # ì´ìƒí•œ íŠ¹ìˆ˜ë¬¸ì ì¡°í•© ê²€ì¶œ
        weird_chars_pattern = r'[^\uac00-\ud7af\u1100-\u11ff\u3130-\u318f\ua960-\ua97f\ud7b0-\ud7ff\s\w\d.,!?""''\(\)\[\]{}:;~â€¦â€”â€“\'\"''""\n\r\t-]'
        
        # í…ìŠ¤íŠ¸ ì •ë¦¬
        cleaned_text = text.strip()
        
        # íŠ¹ìˆ˜ ì—”í‹°í‹° ê²€ì¶œ ë° ì œê±°
        if re.search(special_entity_pattern, cleaned_text):
            print(f"âš ï¸  íŠ¹ìˆ˜ ì—”í‹°í‹° ë¬¸ì ê°ì§€ë¨ (&O;, &C; ë“±), ì¬ë²ˆì—­ í•„ìš”")
            return None
            
        # HTML ì—”í‹°í‹° ê²€ì¶œ
        if re.search(html_entity_pattern, cleaned_text):
            print(f"âš ï¸  HTML ì—”í‹°í‹° ê°ì§€ë¨, ì¬ë²ˆì—­ í•„ìš”")
            return None
        
        # ì¤‘êµ­ì–´ ë¬¸ì ê²€ì¶œ
        if re.search(chinese_pattern, cleaned_text):
            print(f"âš ï¸  ì¤‘êµ­ì–´ ë¬¸ì ê°ì§€ë¨, ì¬ë²ˆì—­ í•„ìš”")
            return None
        
        # ì¼ë³¸ì–´ ë¬¸ì ê²€ì¶œ
        if re.search(japanese_pattern, cleaned_text):
            print(f"âš ï¸  ì¼ë³¸ì–´ ë¬¸ì ê°ì§€ë¨, ì¬ë²ˆì—­ í•„ìš”")
            return None
        
        # ê¸°íƒ€ ì´ìƒí•œ ë¬¸ìë“¤ ê²€ì¶œ
        weird_matches = re.findall(weird_chars_pattern, cleaned_text)
        if weird_matches:
            print(f"âš ï¸  ë¹„ì •ìƒ ë¬¸ì ê°ì§€ë¨: {set(weird_matches)}, ì¬ë²ˆì—­ í•„ìš”")
            return None
            
        return cleaned_text
    
    def _get_fallback_options(self, attempt: int) -> dict:
        """ì‹¤íŒ¨ ì‹œ ì‹œë„í•  ë‹¤ì–‘í•œ ëª¨ë¸ ì˜µì…˜ë“¤"""
        fallback_configs = [
            # ê¸°ë³¸ ì„¤ì •
            {
                'temperature': self.temperature,
                'top_p': 0.8,
                'top_k': 30,
                'repeat_penalty': 1.2,
                'seed': 42
            },
            # ë” ë³´ìˆ˜ì ì¸ ì„¤ì •
            {
                'temperature': 0.05,
                'top_p': 0.6,
                'top_k': 20,
                'repeat_penalty': 1.3,
                'seed': 123
            },
            # ë‹¤ë¥¸ ì‹œë“œì™€ ë§¤ê°œë³€ìˆ˜
            {
                'temperature': 0.2,
                'top_p': 0.7,
                'top_k': 25,
                'repeat_penalty': 1.1,
                'seed': 456
            },
            # ìµœëŒ€í•œ ë³´ìˆ˜ì 
            {
                'temperature': 0.01,
                'top_p': 0.5,
                'top_k': 15,
                'repeat_penalty': 1.4,
                'seed': 789
            },
            # ë‹¤ë¥¸ ì ‘ê·¼ë²•
            {
                'temperature': 0.15,
                'top_p': 0.9,
                'top_k': 35,
                'repeat_penalty': 1.0,
                'seed': 999
            }
        ]
        
        # ì‹œë„ íšŸìˆ˜ì— ë”°ë¼ ë‹¤ë¥¸ ì„¤ì • ì‚¬ìš©
        config_index = min(attempt, len(fallback_configs) - 1)
        return fallback_configs[config_index]

    def translate_text(self, text: str) -> Optional[str]:
        """ë‹¨ì¼ í…ìŠ¤íŠ¸ ë¸”ë¡ ë²ˆì—­ (ë‹¤ì–‘í•œ ì˜µì…˜ìœ¼ë¡œ ì¬ì‹œë„)"""
        if not text.strip():
            return ""
        
        # ìºì‹œ í™•ì¸
        cached_translation = self._check_cache(text)
        if cached_translation:
            return cached_translation
        
        # í•œêµ­ì–´ ì „ìš© ê°•í™” í”„ë¡¬í”„íŠ¸ ìƒì„±
        enhanced_prompt = f"""System: You are a professional Korean translator. Translate ONLY into Korean (Hangul). 

ğŸš¨ ABSOLUTELY FORBIDDEN:
- Chinese characters (æ±‰å­—, ä¸­æ–‡)
- Japanese hiragana/katakana (ã²ã‚‰ãŒãª, ã‚«ã‚¿ã‚«ãƒŠ) 
- HTML entities (&amp;, &lt;, &gt;, etc.)
- Special entities (&O;, &C;, &X; etc.)
- Any non-Korean characters except basic punctuation

âœ… ALLOWED ONLY:
- Korean Hangul characters (í•œê¸€)
- Basic punctuation (.,!?""'' etc.)
- Numbers and English letters only if absolutely necessary
- Standard quotation marks

{self.translation_prompt.format(text=text.strip())}

ğŸ”¥ FINAL CHECK: Your output must contain ONLY Korean characters and basic punctuation. NO strange symbols, entities, or foreign characters!"""
        
        # ìµœëŒ€ ì‹œë„ íšŸìˆ˜ë¥¼ ëŠ˜ë ¤ì„œ ë‹¤ì–‘í•œ ì˜µì…˜ ì‹œë„
        max_attempts = max(self.max_retries, 5)
        last_translation = None  # ë§ˆì§€ë§‰ìœ¼ë¡œ ë°›ì€ ë²ˆì—­ (ì‹¤íŒ¨í•œ ê²ƒì´ë¼ë„)
        
        for attempt in range(max_attempts):
            try:
                # ì‹œë„ë§ˆë‹¤ ë‹¤ë¥¸ ì˜µì…˜ ì‚¬ìš©
                options = self._get_fallback_options(attempt)
                
                # GPU ë ˆì´ì–´ ì„¤ì •ì´ ìˆìœ¼ë©´ ì¶”ê°€
                if self.num_gpu_layers is not None:
                    options['num_gpu'] = self.num_gpu_layers
                
                print(f"   ì‹œë„ {attempt + 1}: temp={options['temperature']}, top_p={options['top_p']}, seed={options['seed']}")
                
                # Ollama Python í´ë¼ì´ì–¸íŠ¸ë¡œ ë²ˆì—­ ìš”ì²­
                response = self.client.generate(
                    model=self.model_name,
                    prompt=enhanced_prompt,
                    options=options
                )
                
                translation = response.get('response', '').strip()
                if translation:
                    # ë²ˆì—­ì„ ë°›ì•˜ìœ¼ë¯€ë¡œ ì €ì¥ (ê²€ì¦ ì „ì—ë„)
                    last_translation = translation
                    
                    # í•œêµ­ì–´ ì™¸ ì–¸ì–´ ê²€ì¦
                    validated_translation = self._validate_korean_only(translation)
                    if validated_translation is None:
                        print(f"âš ï¸  í•œêµ­ì–´ ì™¸ ì–¸ì–´ ê°ì§€, ë‹¤ë¥¸ ì˜µì…˜ìœ¼ë¡œ ì¬ì‹œë„")
                        # 2ë²ˆ ì‹¤íŒ¨í–ˆìœ¼ë©´ ì‹¤íŒ¨í•œ ë²ˆì—­ì´ë¼ë„ ì‚¬ìš©
                        if attempt >= 1:  # 2ë²ˆì§¸ ì‹œë„ë¶€í„° (0, 1ë²ˆì§¸ë©´ 2ë²ˆ ì‹¤íŒ¨)
                            print(f"âš ï¸  2ë²ˆ ì‹¤íŒ¨ í›„ì´ë¯€ë¡œ ë¬¸ì œê°€ ìˆëŠ” ë²ˆì—­ì´ë¼ë„ ì‚¬ìš©: {translation[:100]}...")
                            # ìºì‹œì—ëŠ” ì €ì¥í•˜ì§€ ì•ŠìŒ (ë¬¸ì œê°€ ìˆëŠ” ë²ˆì—­ì´ë¯€ë¡œ)
                            with self.stats_lock:
                                self.stats["total_translations"] += 1
                            return translation
                        continue  # ë‹¤ë¥¸ ì˜µì…˜ìœ¼ë¡œ ì¬ì‹œë„
                    
                    # ìºì‹œì— ì €ì¥
                    self._save_to_cache(text, validated_translation)
                    with self.stats_lock:
                        self.stats["total_translations"] += 1
                    print(f"âœ… ì„±ê³µ! (ì‹œë„ {attempt + 1})")
                    return validated_translation
                else:
                    print(f"ê²½ê³ : ë¹ˆ ë²ˆì—­ ê²°ê³¼ (ì‹œë„ {attempt + 1}/{max_attempts})")
                    
            except ollama.ResponseError as e:
                print(f"Ollama ì‘ë‹µ ì˜¤ë¥˜: {e} (ì‹œë„ {attempt + 1}/{self.max_retries})")
                if "model" in str(e).lower() and "not found" in str(e).lower():
                    print(f"ëª¨ë¸ '{self.model_name}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    print("ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: ollama list")
                    break
            except ollama.RequestError as e:
                print(f"Ollama ìš”ì²­ ì˜¤ë¥˜: {e} (ì‹œë„ {attempt + 1}/{self.max_retries})")
                if "connection" in str(e).lower():
                    print("Ollama ì„œë¹„ìŠ¤ê°€ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                    print("ì„œë¹„ìŠ¤ ì‹œì‘: ollama serve")
                    break
            except Exception as e:
                print(f"ë²ˆì—­ ì˜¤ë¥˜: {e} (ì‹œë„ {attempt + 1}/{self.max_retries})")
            
            if attempt < self.max_retries - 1:
                time.sleep(2 ** attempt)  # ì§€ìˆ˜ ë°±ì˜¤í”„
        
        # ëª¨ë“  ì‹œë„ê°€ ì‹¤íŒ¨í–ˆì§€ë§Œ ë§ˆì§€ë§‰ ë²ˆì—­ì´ ìˆìœ¼ë©´ ê·¸ê²ƒì„ ì‚¬ìš©
        if last_translation:
            print(f"âš ï¸  ëª¨ë“  ê²€ì¦ ì‹¤íŒ¨, í•˜ì§€ë§Œ ë§ˆì§€ë§‰ ë²ˆì—­ ì‚¬ìš©: {last_translation[:100]}...")
            with self.stats_lock:
                self.stats["total_translations"] += 1
            return last_translation
        
        print(f"âŒ {self.max_retries}ë²ˆ ì‹œë„ í›„ ë²ˆì—­ ì™„ì „ ì‹¤íŒ¨")
        return None
    
    def translate_batch(self, texts: List[str]) -> List[Optional[str]]:
        """ë°°ì¹˜ í…ìŠ¤íŠ¸ ë²ˆì—­"""
        results = []
        for text in texts:
            result = self.translate_text(text)
            results.append(result)
        return results
    
    def _translate_chunk_worker(self, chunk_info: Dict, chunks_dir: Path, 
                                translated_chunks_dir: Path, progress: Dict, 
                                progress_file: Path, pbar: tqdm) -> Tuple[str, bool]:
        """ë‹¨ì¼ ì²­í¬ ë²ˆì—­ ì›Œì»¤"""
        chunk_file = chunk_info["file"]
        
        # ì´ë¯¸ ì™„ë£Œëœ ì²­í¬ ê±´ë„ˆë›°ê¸°
        if chunk_file in progress.get("completed", []):
            return chunk_file, True
        
        # ì²­í¬ íŒŒì¼ ì½ê¸°
        chunk_path = chunks_dir / chunk_file
        if not chunk_path.exists():
            print(f"ê²½ê³ : ì²­í¬ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {chunk_file}")
            with self.stats_lock:
                progress.setdefault("failed", []).append(chunk_file)
            return chunk_file, False
        
        try:
            with open(chunk_path, 'r', encoding='utf-8') as f:
                original_text = f.read()
            
            # ë²ˆì—­ ìˆ˜í–‰
            translated_text = self.translate_text(original_text)
            
            if translated_text:
                # ë²ˆì—­ ê²°ê³¼ ì €ì¥
                output_file = translated_chunks_dir / f"ko_{chunk_file}"
                with open(output_file, 'w', encoding='utf-8') as f:
                    f.write(translated_text)
                
                # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
                with self.stats_lock:
                    progress.setdefault("completed", []).append(chunk_file)
                    if chunk_file in progress.get("failed", []):
                        progress["failed"].remove(chunk_file)
                    pbar.update(1)
                
                return chunk_file, True
            else:
                # ì‹¤íŒ¨ ê¸°ë¡
                with self.stats_lock:
                    progress.setdefault("failed", []).append(chunk_file)
                print(f"ì‹¤íŒ¨: {chunk_file}")
                return chunk_file, False
                
        except Exception as e:
            print(f"ì˜¤ë¥˜ ì²˜ë¦¬ ì¤‘ {chunk_file}: {e}")
            with self.stats_lock:
                progress.setdefault("failed", []).append(chunk_file)
            return chunk_file, False
    
    def translate_chunks(self, input_dir: str, output_dir: str, use_parallel: bool = True) -> Dict[str, any]:
        """ì²­í¬ ë””ë ‰í† ë¦¬ ì „ì²´ ë²ˆì—­ (ë³‘ë ¬ ì²˜ë¦¬ ì§€ì›)"""
        input_path = Path(input_dir)
        output_path = Path(output_dir)
        
        # ì…ë ¥ ë””ë ‰í† ë¦¬ í™•ì¸
        if not input_path.exists():
            raise FileNotFoundError(f"ì…ë ¥ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_dir}")
        
        chunks_dir = input_path / "chunks"
        if not chunks_dir.exists():
            raise FileNotFoundError(f"ì²­í¬ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {chunks_dir}")
        
        # ì²­í¬ ì¸ë±ìŠ¤ ë¡œë“œ
        chunk_index_file = chunks_dir / "chunk_index.json"
        if not chunk_index_file.exists():
            raise FileNotFoundError(f"ì²­í¬ ì¸ë±ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {chunk_index_file}")
        
        with open(chunk_index_file, 'r', encoding='utf-8') as f:
            chunk_index = json.load(f)
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        output_path.mkdir(parents=True, exist_ok=True)
        translated_chunks_dir = output_path / "translated_chunks"
        translated_chunks_dir.mkdir(exist_ok=True)
        
        # ë²ˆì—­ ì§„í–‰ ìƒí™© íŒŒì¼
        progress_file = output_path / "translation_progress.json"
        progress = self._load_progress(progress_file)
        
        # ë²ˆì—­ í†µê³„
        stats = {
            "total_chunks": len(chunk_index["chunks"]),
            "completed": len(progress.get("completed", [])),
            "failed": len(progress.get("failed", [])),
            "start_time": time.time(),
            "model_name": self.model_name,
            "parallel_mode": use_parallel,
            "max_workers": self.max_workers if use_parallel else 1
        }
        
        # ìë™ ì¥ë¥´ ê°ì§€ (í•„ìš”í•œ ê²½ìš°)
        if self.auto_detect_genre and len(chunk_index["chunks"]) > 0:
            print("ğŸ” ì¥ë¥´ ìë™ ê°ì§€ ì¤‘...")
            
            # ì²« ë²ˆì§¸ ì²­í¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¥ë¥´ ê°ì§€
            first_chunk_file = chunk_index["chunks"][0]["file"]
            first_chunk_path = chunks_dir / first_chunk_file
            
            try:
                with open(first_chunk_path, 'r', encoding='utf-8') as f:
                    sample_text = f.read()
                
                detected_genre = self.detect_genre_from_text(sample_text)
                
                if detected_genre != self.genre:
                    print(f"ğŸ“š ê°ì§€ëœ ì¥ë¥´: {detected_genre} (ê¸°ë³¸ê°’ {self.genre}ì—ì„œ ë³€ê²½)")
                    self.genre = detected_genre
                    # í”„ë¡¬í”„íŠ¸ ì—…ë°ì´íŠ¸
                    from .prompts import get_translation_prompt
                    self.translation_prompt = get_translation_prompt(self.genre)
                else:
                    print(f"ğŸ“š ì¥ë¥´: {self.genre} (ìë™ ê°ì§€ë¡œ í™•ì¸ë¨)")
            except Exception as e:
                print(f"âš ï¸  ì¥ë¥´ ê°ì§€ ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©: {self.genre}")
        else:
            print(f"ğŸ“š ì¥ë¥´: {self.genre} (ì‚¬ìš©ì ì§€ì •)")

        print(f"ğŸ“š ë²ˆì—­ ì‹œì‘: {stats['total_chunks']}ê°œ ì²­í¬")
        print(f"ğŸ¤– ëª¨ë¸: {self.model_name}")
        print(f"âš¡ ë³‘ë ¬ ì²˜ë¦¬: {'í™œì„±í™”' if use_parallel else 'ë¹„í™œì„±í™”'} (ì›Œì»¤: {stats['max_workers']})")
        print(f"âœ… ì™„ë£Œ: {stats['completed']}ê°œ")
        print(f"âŒ ì‹¤íŒ¨: {stats['failed']}ê°œ")
        if self.enable_cache:
            print(f"ğŸ’¾ ìºì‹±: í™œì„±í™”")
        print("=" * 50)
        
        # ì§„í–‰ë°” ì„¤ì • (macOS zsh + oh-my-zsh í˜¸í™˜)
        # í„°ë¯¸ë„ í™˜ê²½ ì²´í¬
        term = os.environ.get('TERM', '')
        is_dumb_terminal = term in ['dumb', ''] or os.environ.get('CI') == 'true'
        
        pbar = tqdm(
            total=len(chunk_index["chunks"]),
            desc="ë²ˆì—­ ì§„í–‰",
            initial=stats['completed'],
            ncols=80,  # í„°ë¯¸ë„ ë„ˆë¹„ ê³ ì •
            ascii=True,  # ASCII ë¬¸ì ì‚¬ìš© (ìœ ë‹ˆì½”ë“œ ë¬¸ì œ ë°©ì§€)
            bar_format='{desc}: {percentage:3.0f}%|{bar:20}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]',
            disable=is_dumb_terminal,  # dumb í„°ë¯¸ë„ì—ì„œëŠ” ë¹„í™œì„±í™”
            dynamic_ncols=False,  # ë™ì  ë„ˆë¹„ ë¹„í™œì„±í™”
            leave=True,  # ì™„ë£Œ í›„ì—ë„ ì§„í–‰ë°” ìœ ì§€
            mininterval=0.5,  # ì—…ë°ì´íŠ¸ ìµœì†Œ ê°„ê²© (0.5ì´ˆ)
            maxinterval=2.0   # ì—…ë°ì´íŠ¸ ìµœëŒ€ ê°„ê²© (2ì´ˆ)
        )
        
        if use_parallel:
            # ë³‘ë ¬ ì²˜ë¦¬
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                # ë¯¸ì™„ë£Œ ì²­í¬ë§Œ ì¶”ì¶œ
                pending_chunks = [
                    chunk for chunk in chunk_index["chunks"]
                    if chunk["file"] not in progress.get("completed", [])
                ]
                
                # ë°°ì¹˜ë³„ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
                for i in range(0, len(pending_chunks), self.batch_size):
                    batch = pending_chunks[i:i + self.batch_size]
                    
                    # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì‘ì—… ì œì¶œ
                    futures = {
                        executor.submit(
                            self._translate_chunk_worker,
                            chunk_info, chunks_dir, translated_chunks_dir,
                            progress, progress_file, pbar
                        ): chunk_info for chunk_info in batch
                    }
                    
                    # ì™„ë£Œëœ ì‘ì—… ì²˜ë¦¬
                    for future in as_completed(futures):
                        chunk_file, success = future.result()
                        if success:
                            stats["completed"] = len(progress.get("completed", []))
                        else:
                            stats["failed"] = len(progress.get("failed", []))
                        
                        # ì§„í–‰ ìƒí™© ì €ì¥ (ë°°ì¹˜ë§ˆë‹¤)
                        self._save_progress(progress_file, progress)
        else:
            # ìˆœì°¨ ì²˜ë¦¬ (ê¸°ì¡´ ë°©ì‹)
            for chunk_info in chunk_index["chunks"]:
                chunk_file, success = self._translate_chunk_worker(
                    chunk_info, chunks_dir, translated_chunks_dir,
                    progress, progress_file, pbar
                )
                if success:
                    stats["completed"] = len(progress.get("completed", []))
                else:
                    stats["failed"] = len(progress.get("failed", []))
                
                # ì§„í–‰ ìƒí™© ì €ì¥
                self._save_progress(progress_file, progress)
        
        pbar.close()
        
        # ë²ˆì—­ ì™„ë£Œ í†µê³„
        stats["end_time"] = time.time()
        stats["duration"] = stats["end_time"] - stats["start_time"]
        
        # ìºì‹œ í†µê³„ ì¶”ê°€
        if self.enable_cache:
            stats["cache_stats"] = {
                "hits": self.stats["cache_hits"],
                "misses": self.stats["cache_misses"],
                "hit_rate": (self.stats["cache_hits"] / 
                           (self.stats["cache_hits"] + self.stats["cache_misses"]) * 100
                           if (self.stats["cache_hits"] + self.stats["cache_misses"]) > 0 else 0)
            }
        
        # ë²ˆì—­ ì¸ë±ìŠ¤ ìƒì„±
        self._create_translation_index(output_path, chunk_index, stats)
        
        return stats
    
    def fix_translated_chunks(self, translated_dir: str) -> Dict[str, any]:
        """ë²ˆì—­ëœ ì²­í¬ë“¤ì—ì„œ ë¬¸ì œê°€ ìˆëŠ” ë¶€ë¶„ì„ ê°ì§€í•˜ê³  ì¬ë²ˆì—­"""
        translated_path = Path(translated_dir)
        
        if not translated_path.exists():
            raise FileNotFoundError(f"ë²ˆì—­ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {translated_dir}")
        
        translated_chunks_dir = translated_path / "translated_chunks"
        if not translated_chunks_dir.exists():
            raise FileNotFoundError(f"ë²ˆì—­ëœ ì²­í¬ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {translated_chunks_dir}")
        
        # ë²ˆì—­ëœ íŒŒì¼ë“¤ ê²€ì‚¬
        ko_files = list(translated_chunks_dir.glob("ko_*.txt"))
        
        stats = {
            "total_files": len(ko_files),
            "problem_files": [],
            "fixed_files": [],
            "failed_fixes": [],
            "start_time": time.time()
        }
        
        print(f"ğŸ” ë²ˆì—­ëœ íŒŒì¼ ê²€ì‚¬ ì¤‘: {len(ko_files)}ê°œ íŒŒì¼")
        print("=" * 50)
        
        pbar = tqdm(ko_files, desc="ë¬¸ì œ íŒŒì¼ ê²€ì‚¬", ncols=80, ascii=True)
        
        for ko_file in pbar:
            try:
                with open(ko_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # ë¬¸ì œ ìˆëŠ” ë¶€ë¶„ ê°ì§€
                problems = self._detect_translation_problems(content)
                
                if problems:
                    stats["problem_files"].append({
                        "file": ko_file.name,
                        "problems": problems
                    })
                    
                    pbar.set_description(f"ì¬ë²ˆì—­: {ko_file.name}")
                    
                    # ì›ë³¸ ì²­í¬ íŒŒì¼ ì°¾ê¸°
                    original_file = ko_file.name.replace("ko_", "")
                    original_path = None
                    
                    # ì›ë³¸ ì²­í¬ ë””ë ‰í† ë¦¬ì—ì„œ ì°¾ê¸°
                    possible_paths = [
                        translated_path.parent / "chunks" / original_file,
                        translated_path / "chunks" / original_file,
                        Path(translated_dir).parent / original_file.replace("_", "/").replace(".txt", "") / "chunks" / original_file
                    ]
                    
                    for path in possible_paths:
                        if path.exists():
                            original_path = path
                            break
                    
                    if original_path and original_path.exists():
                        # ì›ë³¸ í…ìŠ¤íŠ¸ ì½ê¸°
                        with open(original_path, 'r', encoding='utf-8') as f:
                            original_text = f.read()
                        
                        print(f"\nğŸ”§ ì¬ë²ˆì—­ ì‹œë„: {ko_file.name}")
                        print(f"   ê°ì§€ëœ ë¬¸ì œ: {', '.join(problems)}")
                        
                        # ì¬ë²ˆì—­ ìˆ˜í–‰
                        new_translation = self.translate_text(original_text)
                        
                        if new_translation:
                            # ë²ˆì—­ì„ ë°›ì•˜ìœ¼ë©´ íŒŒì¼ ì €ì¥ (ì™„ë²½í•˜ì§€ ì•Šì•„ë„)
                            with open(ko_file, 'w', encoding='utf-8') as f:
                                f.write(new_translation)
                            
                            # ê²€ì¦ì„ í†µê³¼í–ˆëŠ”ì§€ í™•ì¸
                            validated = self._validate_korean_only(new_translation)
                            if validated:
                                stats["fixed_files"].append(ko_file.name)
                                print(f"âœ… ì¬ë²ˆì—­ ì™„ë£Œ: {ko_file.name}")
                            else:
                                stats["fixed_files"].append(ko_file.name)
                                print(f"âš ï¸  ì¬ë²ˆì—­ ì™„ë£Œ (ë¬¸ì œ ìˆìŒ): {ko_file.name}")
                                print(f"   â†’ 2ë²ˆ ì‹¤íŒ¨ í›„ ë¬¸ì œê°€ ìˆëŠ” ë²ˆì—­ì´ë¼ë„ ì €ì¥í•¨")
                        else:
                            stats["failed_fixes"].append(ko_file.name)
                            print(f"âŒ ì¬ë²ˆì—­ ì™„ì „ ì‹¤íŒ¨: {ko_file.name}")
                    else:
                        print(f"âš ï¸  ì›ë³¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {original_file}")
                        stats["failed_fixes"].append(ko_file.name)
                        
            except Exception as e:
                print(f"ì˜¤ë¥˜ ì²˜ë¦¬ ì¤‘ {ko_file.name}: {e}")
                stats["failed_fixes"].append(ko_file.name)
        
        pbar.close()
        
        # ê²°ê³¼ ì¶œë ¥
        stats["end_time"] = time.time()
        stats["duration"] = stats["end_time"] - stats["start_time"]
        
        print("\n" + "=" * 50)
        print("ğŸ”§ ë¶€ë¶„ ì¬ë²ˆì—­ ì™„ë£Œ!")
        print(f"ì´ íŒŒì¼: {stats['total_files']}ê°œ")
        print(f"ë¬¸ì œ íŒŒì¼: {len(stats['problem_files'])}ê°œ")
        print(f"ìˆ˜ì • ì™„ë£Œ: {len(stats['fixed_files'])}ê°œ")
        print(f"ìˆ˜ì • ì‹¤íŒ¨: {len(stats['failed_fixes'])}ê°œ")
        print(f"ì†Œìš” ì‹œê°„: {stats['duration'] / 60:.1f}ë¶„")
        
        return stats
    
    def _detect_translation_problems(self, text: str) -> List[str]:
        """ë²ˆì—­ í…ìŠ¤íŠ¸ì—ì„œ ë¬¸ì œì ë“¤ì„ ê°ì§€"""
        import re
        problems = []
        
        # ì¤‘êµ­ì–´ ë¬¸ì ê²€ì¶œ
        if re.search(r'[\u4e00-\u9fff]', text):
            problems.append("ì¤‘êµ­ì–´ ë¬¸ì")
        
        # ì¼ë³¸ì–´ ë¬¸ì ê²€ì¶œ
        if re.search(r'[\u3040-\u309f\u30a0-\u30ff]', text):
            problems.append("ì¼ë³¸ì–´ ë¬¸ì")
        
        # íŠ¹ìˆ˜ ì—”í‹°í‹° ê²€ì¶œ
        if re.search(r'&[A-Z]+;', text):
            problems.append("íŠ¹ìˆ˜ ì—”í‹°í‹°")
        
        # HTML ì—”í‹°í‹° ê²€ì¶œ
        if re.search(r'&[a-zA-Z0-9#]+;', text):
            problems.append("HTML ì—”í‹°í‹°")
        
        # ì´ìƒí•œ íŠ¹ìˆ˜ë¬¸ì ê²€ì¶œ
        weird_chars = re.findall(r'[^\uac00-\ud7af\u1100-\u11ff\u3130-\u318f\ua960-\ua97f\ud7b0-\ud7ff\s\w\d.,!?""''\(\)\[\]{}:;~â€¦â€”â€“\'\"''""\n\r\t-]', text)
        if weird_chars:
            problems.append("ë¹„ì •ìƒ ë¬¸ì")
        
        # ë¹ˆ ë²ˆì—­ì´ë‚˜ ë„ˆë¬´ ì§§ì€ ë²ˆì—­
        if len(text.strip()) < 10:
            problems.append("ë¶ˆì™„ì „í•œ ë²ˆì—­")
        
        # ì˜ì–´ê°€ ë„ˆë¬´ ë§ì´ ë‚¨ì•„ìˆëŠ” ê²½ìš° (í•œê¸€ ë¹„ìœ¨ì´ 50% ë¯¸ë§Œ)
        korean_chars = len(re.findall(r'[\uac00-\ud7af]', text))
        total_chars = len(re.sub(r'\s', '', text))
        if total_chars > 0 and korean_chars / total_chars < 0.5:
            problems.append("ë²ˆì—­ ë¶ˆì¶©ë¶„")
        
        return problems
    
    def _load_progress(self, progress_file: Path) -> Dict:
        """ë²ˆì—­ ì§„í–‰ ìƒí™© ë¡œë“œ"""
        if progress_file.exists():
            try:
                with open(progress_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception:
                pass
        return {"completed": [], "failed": []}
    
    def _save_progress(self, progress_file: Path, progress: Dict):
        """ë²ˆì—­ ì§„í–‰ ìƒí™© ì €ì¥"""
        try:
            with open(progress_file, 'w', encoding='utf-8') as f:
                json.dump(progress, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"ì§„í–‰ ìƒí™© ì €ì¥ ì˜¤ë¥˜: {e}")
    
    def _create_translation_index(self, output_path: Path, original_index: Dict, stats: Dict):
        """ë²ˆì—­ ì¸ë±ìŠ¤ íŒŒì¼ ìƒì„±"""
        translation_index = {
            "translation_info": {
                "model": self.model_name,
                "temperature": self.temperature,
                "start_time": time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(stats["start_time"])),
                "end_time": time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(stats["end_time"])),
                "duration_minutes": round(stats["duration"] / 60, 2),
                "total_chunks": stats["total_chunks"],
                "completed_chunks": stats["completed"],
                "failed_chunks": stats["failed"]
            },
            "original_info": original_index
        }
        
        index_file = output_path / "translation_index.json"
        with open(index_file, 'w', encoding='utf-8') as f:
            json.dump(translation_index, f, ensure_ascii=False, indent=2)

def main():
    """ë²ˆì—­ê¸° í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Ollama ë²ˆì—­ê¸°")
    parser.add_argument("input_dir", help="ì…ë ¥ ë””ë ‰í† ë¦¬ (ì²­í¬ê°€ ìˆëŠ” ë””ë ‰í† ë¦¬)")
    parser.add_argument("output_dir", help="ì¶œë ¥ ë””ë ‰í† ë¦¬")
    parser.add_argument("--model", default="llama3.1:8b", help="Ollama ëª¨ë¸ëª…")
    parser.add_argument("--temperature", type=float, default=0.1, help="ë²ˆì—­ ì˜¨ë„")
    parser.add_argument("--genre", default="fantasy", choices=["fantasy", "sci-fi", "romance", "mystery", "general"],
                       help="ì†Œì„¤ ì¥ë¥´")
    parser.add_argument("--max-workers", type=int, default=4, help="ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜")
    parser.add_argument("--batch-size", type=int, default=5, help="ë°°ì¹˜ ì²˜ë¦¬ í¬ê¸°")
    parser.add_argument("--no-parallel", action="store_true", help="ë³‘ë ¬ ì²˜ë¦¬ ë¹„í™œì„±í™”")
    parser.add_argument("--no-cache", action="store_true", help="ìºì‹± ë¹„í™œì„±í™”")
    parser.add_argument("--num-gpu-layers", type=int, help="GPUì— ë¡œë“œí•  ë ˆì´ì–´ ìˆ˜")
    
    args = parser.parse_args()
    
    # ë²ˆì—­ê¸° ì´ˆê¸°í™”
    translator = OllamaTranslator(
        model_name=args.model,
        temperature=args.temperature,
        genre=args.genre,
        max_workers=args.max_workers,
        batch_size=args.batch_size,
        enable_cache=not args.no_cache,
        num_gpu_layers=args.num_gpu_layers
    )
    
    # ì—°ê²° í™•ì¸
    if not translator.check_ollama_available():
        print("âŒ Ollama ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("Ollamaê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return
    
    if not translator.check_model_available():
        print(f"âŒ ëª¨ë¸ '{args.model}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ì„ í™•ì¸í•´ì£¼ì„¸ìš”: ollama list")
        return
    
    print("âœ… Ollama ì—°ê²° í™•ì¸ ì™„ë£Œ")
    
    # ë²ˆì—­ ìˆ˜í–‰
    try:
        stats = translator.translate_chunks(
            args.input_dir, 
            args.output_dir,
            use_parallel=not args.no_parallel
        )
        
        print("\n" + "=" * 50)
        print("ğŸ“Š ë²ˆì—­ ì™„ë£Œ!")
        print(f"ì´ ì²­í¬: {stats['total_chunks']}ê°œ")
        print(f"ì™„ë£Œ: {stats['completed']}ê°œ")
        print(f"ì‹¤íŒ¨: {stats['failed']}ê°œ")
        print(f"ì†Œìš” ì‹œê°„: {stats['duration'] / 60:.1f}ë¶„")
        if "cache_stats" in stats:
            print(f"ìºì‹œ íˆíŠ¸ìœ¨: {stats['cache_stats']['hit_rate']:.1f}%")
        print(f"ë²ˆì—­ ê²°ê³¼: {args.output_dir}")
        
    except Exception as e:
        print(f"âŒ ë²ˆì—­ ì˜¤ë¥˜: {e}")

if __name__ == "__main__":
    main()